{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usamathakur87/class-projects/blob/main/Copy_of_RAG_sindh_boys_scout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evjGLA8Jx5k8"
      },
      "outputs": [],
      "source": [
        "# Pakistan zinda bad, we love our country.\n",
        "# 0         1     2    3  4.   5.   6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jgK8e2vq6gvh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "U5a_LR0V7QEY"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "3Q5zbNgD72OA",
        "outputId": "39c6996b-7116-4c12-861e-0c3362ece49c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
              "                    'February 15th, 2025. Move to a newer Gemini version.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
              "                    'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
              "                    'version.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
              "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
              "                    'Move to a newer Gemini version.'),\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
              "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
              "                    'Move to a newer Gemini version.'),\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
              "                    'million tokens.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Pro 002',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in September of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
              "                    'across diverse tasks.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in May of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
              "                    'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
              "                    'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Flash 002',\n",
              "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in September of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B',\n",
              "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
              "                    'Flash model, released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B 001',\n",
              "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
              "                    'Flash model, released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
              "                    'released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
              "                    'smallest and most cost effective Flash model. Replaced by '\n",
              "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
              "       description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
              "                    'smallest and most cost effective Flash model. Replaced by '\n",
              "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-exp',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Experimental',\n",
              "       description='Gemini 2.0 Flash Experimental',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-exp-1206',\n",
              "       base_model_id='',\n",
              "       version='exp_1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-exp-1121',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-exp-1114',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Thinking Experimental',\n",
              "       description='Gemini 2.0 Flash Thinking Experimental',\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Thinking Experimental',\n",
              "       description='Gemini 2.0 Flash Thinking Experimental',\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/learnlm-1.5-pro-experimental',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='LearnLM 1.5 Pro Experimental',\n",
              "       description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
              "                    'mid-size multimodal model that supports up to 2 million tokens.'),\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "list(genai.list_models())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wslvg_cS6hQG",
        "outputId": "ab773c01-0d5b-49fb-a638-8ac45ce23c8b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.03838823,\n",
              " 0.052146558,\n",
              " -0.07062306,\n",
              " -0.037940446,\n",
              " 0.06602876,\n",
              " 0.003412638,\n",
              " 0.011060191,\n",
              " 0.011297473,\n",
              " 0.0088465065,\n",
              " 0.049196165,\n",
              " 0.026362207,\n",
              " 0.029781424,\n",
              " 0.103020616,\n",
              " 0.0018105474,\n",
              " 0.009687083,\n",
              " -0.1145385,\n",
              " 0.053375162,\n",
              " 0.027896093,\n",
              " -0.069912076,\n",
              " 0.033948876,\n",
              " 0.0021454412,\n",
              " -0.06642034,\n",
              " 0.04420987,\n",
              " -0.021766845,\n",
              " -0.05236885,\n",
              " 0.012519431,\n",
              " -0.006010968,\n",
              " -0.005167873,\n",
              " -0.0061494075,\n",
              " 0.0031098027,\n",
              " 0.025889847,\n",
              " 0.056373067,\n",
              " 0.046986956,\n",
              " -0.054916363,\n",
              " 0.02060043,\n",
              " 0.021911908,\n",
              " -0.028752202,\n",
              " 0.022969386,\n",
              " 0.0386514,\n",
              " -0.030484943,\n",
              " -0.089530766,\n",
              " 0.015129875,\n",
              " -0.06397387,\n",
              " 0.047075633,\n",
              " -0.013017894,\n",
              " -0.029871592,\n",
              " -0.0067695505,\n",
              " -0.0074492395,\n",
              " -0.0059240796,\n",
              " 0.03697813,\n",
              " 0.018429233,\n",
              " -0.001360107,\n",
              " -0.011460476,\n",
              " 0.011616342,\n",
              " -0.025911184,\n",
              " -0.05487599,\n",
              " -0.036950245,\n",
              " -0.009039906,\n",
              " 0.011936421,\n",
              " -0.002934238,\n",
              " -0.027310636,\n",
              " 6.6082706e-05,\n",
              " -0.020024965,\n",
              " -0.027493374,\n",
              " 0.002532368,\n",
              " -0.015001376,\n",
              " -0.076121554,\n",
              " 0.032683495,\n",
              " -0.092314504,\n",
              " 0.024527755,\n",
              " -0.0056760833,\n",
              " 0.012404534,\n",
              " -0.054892246,\n",
              " 0.0047428426,\n",
              " 0.027187724,\n",
              " -0.023102688,\n",
              " 0.039532688,\n",
              " -0.04834113,\n",
              " -0.032563046,\n",
              " 0.055642914,\n",
              " -0.06099361,\n",
              " 0.012322463,\n",
              " 0.05332948,\n",
              " 0.044791535,\n",
              " 0.0047311513,\n",
              " 0.03546266,\n",
              " -0.018310213,\n",
              " -0.012843131,\n",
              " -0.03281737,\n",
              " -0.015505913,\n",
              " 0.06564103,\n",
              " 0.004260769,\n",
              " 0.02080681,\n",
              " 0.015551341,\n",
              " 0.03284577,\n",
              " -0.037125297,\n",
              " -0.06909162,\n",
              " -0.12455734,\n",
              " 0.033198744,\n",
              " 0.08457165,\n",
              " -0.014386027,\n",
              " -0.03143208,\n",
              " 0.022384034,\n",
              " -0.004822828,\n",
              " 0.012416186,\n",
              " 0.031697378,\n",
              " -0.1066608,\n",
              " -0.022101557,\n",
              " -0.008450847,\n",
              " -0.0052436255,\n",
              " 0.0067770644,\n",
              " -0.058070153,\n",
              " 0.041524004,\n",
              " -0.028612824,\n",
              " 0.04088967,\n",
              " -0.028240833,\n",
              " -0.034067433,\n",
              " 0.051358417,\n",
              " -0.025877517,\n",
              " 0.028154515,\n",
              " 0.0060459366,\n",
              " 0.026830649,\n",
              " -0.00095973775,\n",
              " 0.017185919,\n",
              " 0.004395111,\n",
              " 0.016280279,\n",
              " 0.0176109,\n",
              " -0.009282771,\n",
              " 0.010105145,\n",
              " -0.03267479,\n",
              " 0.072818495,\n",
              " -0.04778589,\n",
              " 0.01858214,\n",
              " -0.0020002476,\n",
              " -0.05818697,\n",
              " -0.052377947,\n",
              " 0.0286922,\n",
              " -0.014589298,\n",
              " 0.060979374,\n",
              " 0.013860318,\n",
              " -0.0071884138,\n",
              " 0.018080251,\n",
              " -0.07651641,\n",
              " 0.0370387,\n",
              " 0.013180877,\n",
              " -0.021393348,\n",
              " -0.0080846,\n",
              " 0.025077287,\n",
              " -0.06951974,\n",
              " 0.02669913,\n",
              " -0.012492207,\n",
              " 0.01859013,\n",
              " 0.038992975,\n",
              " 0.0051947823,\n",
              " -0.047826,\n",
              " 0.018798502,\n",
              " 0.04019756,\n",
              " -0.039466992,\n",
              " 0.08624624,\n",
              " -0.00062756106,\n",
              " 0.058413763,\n",
              " -0.06800717,\n",
              " 0.029188514,\n",
              " 0.008827655,\n",
              " -0.035301555,\n",
              " -0.025547149,\n",
              " 0.025351042,\n",
              " -0.06396529,\n",
              " -0.024263415,\n",
              " -0.019369712,\n",
              " 0.013893803,\n",
              " -0.02715477,\n",
              " -0.009625998,\n",
              " -0.06395668,\n",
              " -0.058829118,\n",
              " -0.014857789,\n",
              " -0.06285088,\n",
              " -0.019081429,\n",
              " -0.025285693,\n",
              " -0.023807587,\n",
              " 0.10905102,\n",
              " 0.019630715,\n",
              " -0.018424971,\n",
              " -0.051695295,\n",
              " 0.02405902,\n",
              " -0.008708305,\n",
              " 0.013477974,\n",
              " 0.00955763,\n",
              " 0.012669435,\n",
              " 0.052519657,\n",
              " -0.04345961,\n",
              " -0.011683332,\n",
              " -0.01012757,\n",
              " 0.07052994,\n",
              " -0.0055598025,\n",
              " -0.06500708,\n",
              " -0.029050773,\n",
              " -0.020193106,\n",
              " -0.054957137,\n",
              " -0.041815933,\n",
              " 0.029825028,\n",
              " -0.0037500036,\n",
              " -0.04422032,\n",
              " -0.08507198,\n",
              " -0.025203163,\n",
              " -0.022711176,\n",
              " -0.07533748,\n",
              " 0.00038292477,\n",
              " 0.000574007,\n",
              " -0.023213899,\n",
              " -0.05437338,\n",
              " -0.020481981,\n",
              " 0.0309786,\n",
              " -0.03049663,\n",
              " 0.049754348,\n",
              " 0.01301958,\n",
              " 0.059735786,\n",
              " 0.013099316,\n",
              " 0.07402825,\n",
              " 0.0028997941,\n",
              " -0.0031488023,\n",
              " 0.030355373,\n",
              " -0.0345743,\n",
              " 0.027214795,\n",
              " 0.030471744,\n",
              " 0.00409395,\n",
              " -0.006666186,\n",
              " -0.026523612,\n",
              " 0.0037652112,\n",
              " -0.036769453,\n",
              " -0.016134802,\n",
              " 0.033313625,\n",
              " 0.0036026041,\n",
              " 0.016536806,\n",
              " 0.051247668,\n",
              " 0.090472534,\n",
              " 0.010181305,\n",
              " 0.030595109,\n",
              " 0.0070580146,\n",
              " -0.00865541,\n",
              " -0.013374387,\n",
              " 0.010765982,\n",
              " 0.048682828,\n",
              " 0.028028237,\n",
              " -0.020036707,\n",
              " -0.008994092,\n",
              " 0.012286691,\n",
              " 0.040086675,\n",
              " 0.008561659,\n",
              " -0.04989866,\n",
              " -0.03489145,\n",
              " -0.006646721,\n",
              " -0.031492487,\n",
              " -0.021983976,\n",
              " 0.0068252063,\n",
              " -0.055136193,\n",
              " 0.046041407,\n",
              " -0.014977839,\n",
              " 0.018520158,\n",
              " -0.0019085855,\n",
              " 0.036878396,\n",
              " -0.0765386,\n",
              " 0.008701385,\n",
              " -0.059513815,\n",
              " -0.060923666,\n",
              " -0.049235404,\n",
              " 0.004780061,\n",
              " 0.0044634817,\n",
              " 0.08013073,\n",
              " -0.03781426,\n",
              " -0.004155092,\n",
              " -0.07378796,\n",
              " -0.0062083392,\n",
              " 0.00019341962,\n",
              " 0.049237784,\n",
              " 0.016864326,\n",
              " 0.013317688,\n",
              " -0.020757375,\n",
              " 0.011082609,\n",
              " -0.046459932,\n",
              " -0.023551403,\n",
              " -0.0025574379,\n",
              " 0.023846528,\n",
              " -0.03697934,\n",
              " -0.018832024,\n",
              " -0.05554495,\n",
              " 0.017728858,\n",
              " -0.008121325,\n",
              " -0.021077035,\n",
              " -0.006830344,\n",
              " 0.03167001,\n",
              " 0.02122792,\n",
              " 0.053463276,\n",
              " -0.051008683,\n",
              " 0.033604734,\n",
              " 0.0053696907,\n",
              " 0.043874938,\n",
              " 0.0068859165,\n",
              " 0.035119798,\n",
              " -0.009732238,\n",
              " 0.006822771,\n",
              " 0.035985652,\n",
              " 0.010248646,\n",
              " 0.021891562,\n",
              " 0.032525573,\n",
              " 0.011882014,\n",
              " 0.05660894,\n",
              " -0.013528444,\n",
              " -0.013231257,\n",
              " -0.045601033,\n",
              " 0.048486833,\n",
              " 0.061330978,\n",
              " -0.013769018,\n",
              " -0.021071164,\n",
              " -0.05462211,\n",
              " -0.01022884,\n",
              " -0.16075553,\n",
              " 0.0055632093,\n",
              " -0.005055581,\n",
              " -0.0068281684,\n",
              " -0.03442759,\n",
              " 0.02271425,\n",
              " 0.016652118,\n",
              " 0.002752376,\n",
              " 0.043466292,\n",
              " 0.02711752,\n",
              " -0.016791742,\n",
              " -0.048889656,\n",
              " 0.03908195,\n",
              " -0.022985088,\n",
              " 0.012583863,\n",
              " 0.005726204,\n",
              " 0.023895126,\n",
              " -0.05023696,\n",
              " -0.00048284858,\n",
              " 0.008326804,\n",
              " -0.03993198,\n",
              " 0.013789181,\n",
              " 0.047209363,\n",
              " 0.027224733,\n",
              " -0.025249159,\n",
              " -0.0023370015,\n",
              " 0.043043554,\n",
              " 0.021095976,\n",
              " 0.03354987,\n",
              " -0.030191306,\n",
              " 0.02248958,\n",
              " -0.028729452,\n",
              " 0.05607367,\n",
              " -0.009501291,\n",
              " -0.023586508,\n",
              " 0.08498721,\n",
              " 0.0575211,\n",
              " -0.00075478735,\n",
              " -0.011939774,\n",
              " 0.0003264734,\n",
              " 0.060069848,\n",
              " -0.0073553207,\n",
              " 0.033479355,\n",
              " 0.0030075137,\n",
              " -0.01904192,\n",
              " -0.005860383,\n",
              " 0.023575183,\n",
              " 0.05215699,\n",
              " -5.4912136e-05,\n",
              " -0.060488857,\n",
              " -0.023957102,\n",
              " 0.012998325,\n",
              " 0.03517663,\n",
              " -0.035629902,\n",
              " 0.0369,\n",
              " 0.008402149,\n",
              " 0.001765194,\n",
              " 0.0023059861,\n",
              " 0.04675764,\n",
              " -0.04476126,\n",
              " -0.0069735753,\n",
              " -0.0002626759,\n",
              " 0.023913743,\n",
              " -0.02967791,\n",
              " 0.00031710474,\n",
              " -0.010220715,\n",
              " 0.009249499,\n",
              " 0.020453123,\n",
              " -0.03715475,\n",
              " 0.047895074,\n",
              " 0.001733521,\n",
              " 0.0037312186,\n",
              " 0.022507915,\n",
              " 0.06591765,\n",
              " -0.0115289455,\n",
              " 0.013562781,\n",
              " 0.057814762,\n",
              " 0.04246558,\n",
              " 5.7039684e-05,\n",
              " 0.033612076,\n",
              " -0.042697903,\n",
              " 0.0644634,\n",
              " -9.151293e-05,\n",
              " 0.037152737,\n",
              " -0.0011938581,\n",
              " -0.027302312,\n",
              " 0.09659006,\n",
              " 0.0012583006,\n",
              " -0.010336961,\n",
              " -0.061051127,\n",
              " 0.01347184,\n",
              " -0.0049233683,\n",
              " 0.008797497,\n",
              " 0.03161731,\n",
              " -0.03648881,\n",
              " -0.009932623,\n",
              " -0.07835084,\n",
              " 4.2173e-05,\n",
              " -0.03334824,\n",
              " 0.014768315,\n",
              " -0.015830249,\n",
              " 0.021433732,\n",
              " -0.025502263,\n",
              " 0.0038483543,\n",
              " -0.016326146,\n",
              " -0.014174781,\n",
              " -0.016782146,\n",
              " -0.020667074,\n",
              " 0.01614993,\n",
              " -0.08426387,\n",
              " -0.03154883,\n",
              " 0.01654426,\n",
              " 0.0670697,\n",
              " 0.042512123,\n",
              " 0.062353604,\n",
              " -0.030913549,\n",
              " 0.013138386,\n",
              " 0.026835408,\n",
              " 0.0110793365,\n",
              " -0.0218443,\n",
              " 0.026536738,\n",
              " -0.0064630304,\n",
              " -0.022492763,\n",
              " -0.035576217,\n",
              " -0.037078027,\n",
              " 0.01286653,\n",
              " -0.00037669428,\n",
              " 0.04966357,\n",
              " -0.0024665396,\n",
              " 0.0146915335,\n",
              " 0.06098033,\n",
              " -0.029980283,\n",
              " 0.0008697028,\n",
              " 0.064677365,\n",
              " -0.00428359,\n",
              " -0.01547883,\n",
              " -0.025234092,\n",
              " 0.025209295,\n",
              " -0.002039264,\n",
              " 0.03444799,\n",
              " -0.0022913783,\n",
              " 0.023588978,\n",
              " -0.045668975,\n",
              " 0.04170248,\n",
              " -0.044887327,\n",
              " -0.021114206,\n",
              " 0.045086056,\n",
              " 0.015677046,\n",
              " -0.0041803983,\n",
              " -0.02905689,\n",
              " -0.013574269,\n",
              " 0.070722766,\n",
              " -0.0015028456,\n",
              " -0.019128142,\n",
              " -0.026981374,\n",
              " -0.0075038965,\n",
              " 0.009927641,\n",
              " 0.011850352,\n",
              " -0.040748272,\n",
              " 0.029269002,\n",
              " 0.014578034,\n",
              " 0.04644106,\n",
              " -0.06513363,\n",
              " -0.047762387,\n",
              " 0.005994523,\n",
              " -0.016654402,\n",
              " -0.046001986,\n",
              " 0.033784177,\n",
              " -0.0059721963,\n",
              " -0.070746414,\n",
              " -0.016943036,\n",
              " -0.0011437157,\n",
              " 0.029320652,\n",
              " 0.08227695,\n",
              " 0.025052816,\n",
              " 0.0022991449,\n",
              " 0.011448173,\n",
              " -0.008304278,\n",
              " 0.019583032,\n",
              " -0.014954232,\n",
              " 0.030788248,\n",
              " 0.0029797773,\n",
              " -0.03788128,\n",
              " 0.03724129,\n",
              " 0.05528784,\n",
              " -0.034972087,\n",
              " -0.0019545844,\n",
              " -0.05261343,\n",
              " -0.056895804,\n",
              " 0.044773314,\n",
              " -0.063213706,\n",
              " -0.043816824,\n",
              " 0.0905643,\n",
              " -0.04863921,\n",
              " 0.046625808,\n",
              " 0.035758417,\n",
              " 0.011668098,\n",
              " 0.0079242475,\n",
              " -0.03298972,\n",
              " -0.011208986,\n",
              " -0.081154466,\n",
              " -0.0031484948,\n",
              " -0.017065778,\n",
              " 0.024816861,\n",
              " -0.06621424,\n",
              " -0.039437614,\n",
              " 0.042017188,\n",
              " -0.00050168845,\n",
              " 0.018247671,\n",
              " -0.024270063,\n",
              " -0.030748807,\n",
              " 0.02384947,\n",
              " 0.008581831,\n",
              " -0.032525916,\n",
              " -0.003901744,\n",
              " 0.036596827,\n",
              " 0.022501571,\n",
              " 0.014229588,\n",
              " -0.0061629214,\n",
              " 0.047474425,\n",
              " 0.01413304,\n",
              " 0.0031166582,\n",
              " 0.040364627,\n",
              " -0.019387295,\n",
              " 0.014101074,\n",
              " -0.008786798,\n",
              " -0.029672347,\n",
              " 0.02951629,\n",
              " 0.02604202,\n",
              " 0.04345717,\n",
              " -0.060620695,\n",
              " 0.025450386,\n",
              " -0.015682898,\n",
              " 0.028722597,\n",
              " -0.008685122,\n",
              " -0.010794636,\n",
              " -0.012771901,\n",
              " -0.0011217573,\n",
              " 0.005098702,\n",
              " -0.04036902,\n",
              " -0.02358411,\n",
              " -0.0033144718,\n",
              " -0.0679191,\n",
              " -0.020438666,\n",
              " -0.019104637,\n",
              " -0.03788486,\n",
              " 0.0044292295,\n",
              " 0.022491189,\n",
              " 0.0033494795,\n",
              " -0.046392314,\n",
              " 0.03587325,\n",
              " 0.04138297,\n",
              " 0.0287248,\n",
              " -0.0068671093,\n",
              " -0.0005605311,\n",
              " 0.011639575,\n",
              " 0.024345625,\n",
              " -0.060927104,\n",
              " 0.04197428,\n",
              " 0.015342221,\n",
              " -0.018747458,\n",
              " 0.008443097,\n",
              " 0.055946324,\n",
              " -0.005637669,\n",
              " 0.054343686,\n",
              " 0.018478725,\n",
              " 0.037477393,\n",
              " -0.04967184,\n",
              " 0.02941245,\n",
              " 0.010871435,\n",
              " 0.032436363,\n",
              " 0.0059494562,\n",
              " 0.04422599,\n",
              " -0.006740689,\n",
              " -0.02163518,\n",
              " 0.022332964,\n",
              " 0.017083554,\n",
              " -0.0512461,\n",
              " -0.05113343,\n",
              " -0.0320302,\n",
              " -0.048574947,\n",
              " -0.017281223,\n",
              " 0.007265277,\n",
              " -0.025888035,\n",
              " 0.0499955,\n",
              " -0.028761506,\n",
              " 0.047641512,\n",
              " 0.006997258,\n",
              " -0.055606503,\n",
              " -0.117244676,\n",
              " -0.010426683,\n",
              " -0.028365433,\n",
              " -0.018504485,\n",
              " -0.009055795,\n",
              " -0.014238359,\n",
              " -0.015719526,\n",
              " -0.03149241,\n",
              " -0.003400495,\n",
              " -0.012937613,\n",
              " -0.018243145,\n",
              " -0.0144358035,\n",
              " -0.0056894287,\n",
              " 0.054088052,\n",
              " 0.011907005,\n",
              " -0.002125421,\n",
              " -0.01782156,\n",
              " 0.0037183985,\n",
              " -0.054034688,\n",
              " -0.012775008,\n",
              " -0.020216625,\n",
              " -0.023934435,\n",
              " 0.017653301,\n",
              " 0.0046870117,\n",
              " 0.027500125,\n",
              " 0.012845098,\n",
              " 0.022624854,\n",
              " 0.03215435,\n",
              " -0.016278284,\n",
              " 0.017207153,\n",
              " 0.022287257,\n",
              " -0.004011496,\n",
              " -0.021268496,\n",
              " 0.018312145,\n",
              " -0.003118074,\n",
              " -0.0376296,\n",
              " -0.023842245,\n",
              " 0.025278052,\n",
              " 0.0063156355,\n",
              " -0.03015189,\n",
              " 0.031874474,\n",
              " -0.008299831,\n",
              " -0.0011517438,\n",
              " 0.0062523056,\n",
              " -0.00044700058,\n",
              " -0.016225612,\n",
              " -0.032361522,\n",
              " 0.020232819,\n",
              " -0.011365004,\n",
              " -0.025918007,\n",
              " 0.033456538,\n",
              " -0.0038194235,\n",
              " -0.04559527,\n",
              " 0.0091817845,\n",
              " -0.05172906,\n",
              " 0.0042541544,\n",
              " -0.020796109,\n",
              " 0.041575905,\n",
              " 0.022975706,\n",
              " -0.021847228,\n",
              " -0.007324385,\n",
              " -0.011522168,\n",
              " -0.055374276,\n",
              " 0.026773449,\n",
              " -0.020385416,\n",
              " 0.03198812,\n",
              " -0.02696582,\n",
              " 0.02701983,\n",
              " 0.032033257,\n",
              " -0.025706427,\n",
              " -0.028448839,\n",
              " 0.0704481,\n",
              " -0.023736434,\n",
              " -0.047385562,\n",
              " 0.033373505,\n",
              " -0.010090002,\n",
              " -0.027287053,\n",
              " -0.015427962,\n",
              " -0.04851017,\n",
              " 0.036831185,\n",
              " -0.012953718,\n",
              " 0.022658195,\n",
              " -0.021795256,\n",
              " 0.02059936,\n",
              " 0.024475915,\n",
              " 0.00055408303,\n",
              " -0.026111007,\n",
              " -0.03636221,\n",
              " -0.016248602,\n",
              " 0.030392338,\n",
              " 0.07902486,\n",
              " -0.024101842,\n",
              " 0.0055876803,\n",
              " -0.008215156,\n",
              " -0.023136614,\n",
              " -0.030492572,\n",
              " -0.0022403337,\n",
              " -0.035763245,\n",
              " -0.03326536,\n",
              " 0.0021086684,\n",
              " -0.010088928,\n",
              " -0.008121419,\n",
              " 0.0019507741,\n",
              " -0.073499575,\n",
              " 0.049395718,\n",
              " 0.023999428,\n",
              " -0.0007134775,\n",
              " 0.032879937,\n",
              " -0.00640119,\n",
              " 0.02869106,\n",
              " 0.013808199,\n",
              " 0.023221033,\n",
              " -0.01590115,\n",
              " 0.023822999,\n",
              " 0.08655637,\n",
              " -0.04039015,\n",
              " 0.03261114,\n",
              " 0.017652059,\n",
              " 0.03452416,\n",
              " 0.02047702,\n",
              " -0.039462328,\n",
              " -0.04694545,\n",
              " 0.038026884,\n",
              " -0.047012918,\n",
              " 0.074192934,\n",
              " 0.05681196,\n",
              " 0.0008074619,\n",
              " -0.046055894,\n",
              " 0.05716265,\n",
              " -0.019507777,\n",
              " -0.031380996,\n",
              " -0.024576735,\n",
              " -0.0037123791,\n",
              " -0.03486795,\n",
              " 0.02585528,\n",
              " 0.05651957,\n",
              " -0.054238588,\n",
              " -0.06553499,\n",
              " -0.023791313,\n",
              " -0.0122753875,\n",
              " 0.009531079,\n",
              " -0.031291775,\n",
              " -0.045523264,\n",
              " -0.015830627,\n",
              " -0.04805145,\n",
              " -0.059642047,\n",
              " 0.029825516,\n",
              " 0.05650545,\n",
              " 0.019484732,\n",
              " 0.073890746,\n",
              " 0.009196346,\n",
              " 0.030793106,\n",
              " -0.03390429,\n",
              " 0.024764318,\n",
              " 0.061629165,\n",
              " -0.043743283,\n",
              " 0.0106486445,\n",
              " 0.010411605,\n",
              " 0.0031037293,\n",
              " -0.05513253,\n",
              " -0.04937317,\n",
              " 0.004930977,\n",
              " -0.008528327]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "\n",
        "from typing import Dict\n",
        "\n",
        "result : Dict = genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    content=\"Pakistan zinda bad we love our country\",\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of single string\",\n",
        ")\n",
        "\n",
        "# # 1 input > 1 vector output\n",
        "# print(str(result[\"embedding\"])[:50], \"... TRIMMED]\")\n",
        "\n",
        "result['embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LRebzTJABUh",
        "outputId": "15df8926-f07b-487e-ac45-6670e1e3f893"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "len(result['embedding'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whzpN01Z9hPA",
        "outputId": "d303fe85-1b98-4796-8604-bf28fe18437f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "len(result['embedding'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "1ZqSNYKA84oQ",
        "outputId": "ab1d549a-5544-49b8-842c-3fda28998d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.036453027, 0.033254996, -0.03970925, -0.002628 ... TRIMMED ... 768\n",
            "[-0.01591948, 0.032582663, -0.081024624, -0.011298 ... TRIMMED ... 768\n",
            "[0.00037063024, 0.03763057, -0.122695684, -0.00951 ... TRIMMED ... 768\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from typing import Dict\n",
        "\n",
        "result : Dict = genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    content=[\n",
        "        \"What is the meaning of life?\",\n",
        "        \"How much wood would a woodchuck chuck?\",\n",
        "        \"How does the brain work?\",\n",
        "    ],\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of list of strings\",\n",
        ")\n",
        "\n",
        "# A list of inputs > A list of vectors output\n",
        "for v in result[\"embedding\"]:\n",
        "    print(str(v)[:50], \"... TRIMMED ...\", len(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyrCQBs8AANE"
      },
      "source": [
        "# Building Vector Stores & Retreival using Chroma DB and Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GPpBBX9g-PKl"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "w4Wd_L8zAJTn"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "uxHZV_umADw5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
        "        metadata={\"source\": \"fish-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
        "        metadata={\"source\": \"bird-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4kWhNtJb9z5S"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "4b6yLMNR9qOn"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\",\n",
        "                                          google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "# embeddings.embed_query(\"What's our Q1 revenue?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.embed_query(\"What's our Q1 revenue?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRybEBmkp5LX",
        "outputId": "2d0f433d-55da-47b7-e6e0-ead4894f1ce0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.040674030780792236,\n",
              " 0.006255019456148148,\n",
              " -0.013568978756666183,\n",
              " -0.0003686861018650234,\n",
              " 0.04303165152668953,\n",
              " 0.04935013875365257,\n",
              " -0.013514830730855465,\n",
              " -0.027903610840439796,\n",
              " -0.03995805233716965,\n",
              " -0.006844368763267994,\n",
              " 0.0013024156214669347,\n",
              " -0.009539234451949596,\n",
              " 0.0705987736582756,\n",
              " -0.009862210601568222,\n",
              " 0.03167127072811127,\n",
              " -0.02663198858499527,\n",
              " -0.018167555332183838,\n",
              " -0.005245935637503862,\n",
              " -0.14866198599338531,\n",
              " -0.01596848852932453,\n",
              " 0.02811194583773613,\n",
              " -0.0018506837077438831,\n",
              " -0.025303209200501442,\n",
              " -0.01434125192463398,\n",
              " -0.03104301728308201,\n",
              " -0.07088255137205124,\n",
              " 0.011673162691295147,\n",
              " 0.008746510371565819,\n",
              " 0.003015926806256175,\n",
              " -0.010475549846887589,\n",
              " -6.184780795592815e-05,\n",
              " -0.0014338439796119928,\n",
              " -0.03641575202345848,\n",
              " -0.0519932359457016,\n",
              " -0.02123081497848034,\n",
              " 0.03613690286874771,\n",
              " -0.03694721683859825,\n",
              " 0.06530386954545975,\n",
              " 0.031148776412010193,\n",
              " -0.05865824222564697,\n",
              " -0.033094197511672974,\n",
              " -0.002400598954409361,\n",
              " -0.039360735565423965,\n",
              " 0.001522441511042416,\n",
              " 0.03487030044198036,\n",
              " 0.0026657148264348507,\n",
              " -0.0058933584950864315,\n",
              " 0.020132659003138542,\n",
              " -0.0036536972038447857,\n",
              " 0.008130554109811783,\n",
              " -0.009108034893870354,\n",
              " -0.03555028885602951,\n",
              " -0.014387637376785278,\n",
              " 0.0020767864771187305,\n",
              " -0.047531772404909134,\n",
              " -0.023021064698696136,\n",
              " 0.02736302837729454,\n",
              " -0.026102488860487938,\n",
              " 0.08498480916023254,\n",
              " -0.009469239041209221,\n",
              " 0.0319310761988163,\n",
              " -0.0018667412223294377,\n",
              " 0.059389110654592514,\n",
              " -0.0035042506642639637,\n",
              " 0.020587773993611336,\n",
              " -0.03917128965258598,\n",
              " 0.03360649570822716,\n",
              " 0.015973566100001335,\n",
              " -0.0738484337925911,\n",
              " -0.02538050338625908,\n",
              " -0.02277788706123829,\n",
              " 0.05237537994980812,\n",
              " 0.011328230611979961,\n",
              " -0.03981751948595047,\n",
              " -0.0006608059629797935,\n",
              " -0.008069388568401337,\n",
              " 0.007512678857892752,\n",
              " -0.04804745689034462,\n",
              " 0.03459799289703369,\n",
              " 0.08978776633739471,\n",
              " -0.07167279720306396,\n",
              " 0.022603409364819527,\n",
              " 0.08019706606864929,\n",
              " 0.00940668024122715,\n",
              " 0.03418527916073799,\n",
              " -0.015340480022132397,\n",
              " -0.03552362322807312,\n",
              " -0.03181988373398781,\n",
              " -0.05383983999490738,\n",
              " -0.04403817653656006,\n",
              " 0.05618930980563164,\n",
              " -0.02780766971409321,\n",
              " -0.02107255905866623,\n",
              " -0.018637580797076225,\n",
              " 0.0897323489189148,\n",
              " -0.06604108214378357,\n",
              " -0.09660559147596359,\n",
              " -0.05220397189259529,\n",
              " 0.07147490233182907,\n",
              " 0.06639932841062546,\n",
              " 0.004779261536896229,\n",
              " -0.03112303651869297,\n",
              " 0.0008881306857801974,\n",
              " 0.01845836080610752,\n",
              " 0.0274297297000885,\n",
              " 0.0004622933629434556,\n",
              " -0.03492116928100586,\n",
              " -0.004169788211584091,\n",
              " -0.021554481238126755,\n",
              " -0.02542758919298649,\n",
              " -0.02286696434020996,\n",
              " -0.03512721508741379,\n",
              " 0.0026910000015050173,\n",
              " -0.028166597709059715,\n",
              " 0.004026286769658327,\n",
              " -0.00791264045983553,\n",
              " -0.024778082966804504,\n",
              " 0.015024474821984768,\n",
              " -0.010839731432497501,\n",
              " 0.009473622776567936,\n",
              " 0.056382134556770325,\n",
              " 0.021171119064092636,\n",
              " -0.008079694584012032,\n",
              " 0.025672005489468575,\n",
              " 0.028384380042552948,\n",
              " 0.020231332629919052,\n",
              " 0.0428866408765316,\n",
              " -0.0007346387719735503,\n",
              " -0.03555949404835701,\n",
              " -0.05524420365691185,\n",
              " -0.014625327661633492,\n",
              " 0.009426695294678211,\n",
              " 0.010897071100771427,\n",
              " -0.003945027478039265,\n",
              " 0.022394057363271713,\n",
              " -0.024528535082936287,\n",
              " 0.0876956656575203,\n",
              " 0.04642253741621971,\n",
              " -0.030577486380934715,\n",
              " 0.03828105702996254,\n",
              " 0.03191431239247322,\n",
              " -0.04186468943953514,\n",
              " -0.06275169551372528,\n",
              " 0.014168639667332172,\n",
              " -0.01498769223690033,\n",
              " -0.024832768365740776,\n",
              " 0.027766933664679527,\n",
              " -0.0004999113152734935,\n",
              " -0.025478815659880638,\n",
              " -0.03067292645573616,\n",
              " -0.027004195377230644,\n",
              " 0.005373257678002119,\n",
              " -0.005426143296062946,\n",
              " -0.013760142959654331,\n",
              " 0.047259990125894547,\n",
              " -3.351004352225573e-06,\n",
              " 0.011196448467671871,\n",
              " 0.0331735797226429,\n",
              " 0.04530906677246094,\n",
              " 0.026219498366117477,\n",
              " -0.03507794439792633,\n",
              " 0.013839093036949635,\n",
              " -0.02348119020462036,\n",
              " 0.028220539912581444,\n",
              " -0.039287421852350235,\n",
              " 0.023950839415192604,\n",
              " -0.02938219904899597,\n",
              " -0.003724222769960761,\n",
              " 0.03192543610930443,\n",
              " 0.0069425287656486034,\n",
              " -0.02548467367887497,\n",
              " -0.03700289875268936,\n",
              " -0.053304944187402725,\n",
              " -0.06592298299074173,\n",
              " -0.01140379998832941,\n",
              " 0.07297323644161224,\n",
              " -0.010483955964446068,\n",
              " -0.04604703560471535,\n",
              " -0.12386089563369751,\n",
              " -0.05467037111520767,\n",
              " 0.03335092216730118,\n",
              " 0.013243228197097778,\n",
              " -0.05039249360561371,\n",
              " -0.008832715451717377,\n",
              " 0.06478633731603622,\n",
              " 0.04017411917448044,\n",
              " 0.0036366560962051153,\n",
              " -0.026531219482421875,\n",
              " -0.015908753499388695,\n",
              " 0.006120656616985798,\n",
              " -0.01062320638448,\n",
              " 0.005162350367754698,\n",
              " 0.032537247985601425,\n",
              " -0.042312879115343094,\n",
              " 0.017654040828347206,\n",
              " 0.05063820630311966,\n",
              " 0.042538005858659744,\n",
              " -0.03213687241077423,\n",
              " -0.038917768746614456,\n",
              " 0.010565686970949173,\n",
              " -0.010164313018321991,\n",
              " -0.029124340042471886,\n",
              " 0.028939809650182724,\n",
              " -0.046825725585222244,\n",
              " -0.04041363671422005,\n",
              " -0.02851947396993637,\n",
              " -0.05352668836712837,\n",
              " -0.023540807887911797,\n",
              " -0.05905939266085625,\n",
              " 0.006182074546813965,\n",
              " 0.0236660148948431,\n",
              " 0.01643635518848896,\n",
              " -0.055969204753637314,\n",
              " -0.052113115787506104,\n",
              " 0.005142379552125931,\n",
              " 0.015834525227546692,\n",
              " 0.07765226811170578,\n",
              " 0.014419138431549072,\n",
              " 0.014757545664906502,\n",
              " -0.016682716086506844,\n",
              " 0.035169124603271484,\n",
              " -0.012445286847651005,\n",
              " 0.04094401374459267,\n",
              " -0.0004567909345496446,\n",
              " 0.03075295127928257,\n",
              " 0.015191249549388885,\n",
              " 0.0008787462138570845,\n",
              " 0.011183375492691994,\n",
              " -0.02541988343000412,\n",
              " -0.01335611566901207,\n",
              " 0.04078405350446701,\n",
              " 0.00097758905030787,\n",
              " -0.027971843257546425,\n",
              " 0.03310989588499069,\n",
              " -0.036995869129896164,\n",
              " 0.07041022926568985,\n",
              " 0.038847681134939194,\n",
              " 0.011135242879390717,\n",
              " 0.0178934745490551,\n",
              " -0.06709057837724686,\n",
              " -0.018150683492422104,\n",
              " -0.004681224934756756,\n",
              " -0.020785439759492874,\n",
              " -0.0015118676237761974,\n",
              " -0.01593458652496338,\n",
              " 0.008007141761481762,\n",
              " 0.056374192237854004,\n",
              " 0.018349969759583473,\n",
              " -0.01836358942091465,\n",
              " -0.03326503559947014,\n",
              " -0.062389075756073,\n",
              " 0.012848050333559513,\n",
              " -0.0030290777795016766,\n",
              " 0.00806711707264185,\n",
              " -0.06129119545221329,\n",
              " -0.009198661893606186,\n",
              " 0.0034411519300192595,\n",
              " -0.05829843878746033,\n",
              " 0.017203867435455322,\n",
              " 0.07828714698553085,\n",
              " 0.02788754366338253,\n",
              " -0.05472508445382118,\n",
              " -0.0053860764019191265,\n",
              " -0.04209680110216141,\n",
              " -0.057854652404785156,\n",
              " -0.06215570494532585,\n",
              " -0.037162765860557556,\n",
              " -0.026187805458903313,\n",
              " 0.013729006983339787,\n",
              " 0.00522513035684824,\n",
              " 0.007254887372255325,\n",
              " 0.007314743008464575,\n",
              " -0.044295214116573334,\n",
              " 0.012690248899161816,\n",
              " 0.0015951964305713773,\n",
              " -0.020993180572986603,\n",
              " -0.028973281383514404,\n",
              " 0.01797867938876152,\n",
              " -0.03705243021249771,\n",
              " 0.016344338655471802,\n",
              " 0.047140851616859436,\n",
              " -0.0045312875881791115,\n",
              " 0.020043889060616493,\n",
              " -0.04180380329489708,\n",
              " 0.008862539194524288,\n",
              " 0.011784982867538929,\n",
              " 0.012936141341924667,\n",
              " 0.04847110062837601,\n",
              " 0.020518505945801735,\n",
              " -0.04009382799267769,\n",
              " 0.004235445521771908,\n",
              " -0.015121255069971085,\n",
              " -0.017107676714658737,\n",
              " -0.0022628495935350657,\n",
              " 0.03131377324461937,\n",
              " 0.058326173573732376,\n",
              " 0.047612544149160385,\n",
              " 0.0008990900823846459,\n",
              " 0.012505724094808102,\n",
              " 0.0234519150108099,\n",
              " 0.011617792770266533,\n",
              " 0.0089767687022686,\n",
              " 0.012201692909002304,\n",
              " 0.05818939581513405,\n",
              " 0.07964139431715012,\n",
              " 0.02628134936094284,\n",
              " 0.0035562783014029264,\n",
              " -0.06586579233407974,\n",
              " -0.06682974845170975,\n",
              " -0.004343168810009956,\n",
              " 0.024766702204942703,\n",
              " -0.045907892286777496,\n",
              " -0.024228105321526527,\n",
              " -0.04364803433418274,\n",
              " -0.01721922867000103,\n",
              " -0.014083858579397202,\n",
              " -0.08534359931945801,\n",
              " -0.022292688488960266,\n",
              " -0.035723213106393814,\n",
              " -0.05347568914294243,\n",
              " 0.04860648512840271,\n",
              " -0.024981264024972916,\n",
              " -0.05546209588646889,\n",
              " -0.02599288523197174,\n",
              " 0.059800885617733,\n",
              " 0.027545634657144547,\n",
              " 0.010634008795022964,\n",
              " 0.030378857627511024,\n",
              " -0.06266247481107712,\n",
              " -0.02802923507988453,\n",
              " -0.0164673812687397,\n",
              " -0.00466604670509696,\n",
              " 0.0068775867111980915,\n",
              " -0.04135647043585777,\n",
              " 0.013757770881056786,\n",
              " 0.030090903863310814,\n",
              " -0.051660291850566864,\n",
              " 0.035671450197696686,\n",
              " 0.05833543464541435,\n",
              " 0.030305322259664536,\n",
              " 0.030703585594892502,\n",
              " 0.045433782041072845,\n",
              " 0.035327911376953125,\n",
              " 0.04064740985631943,\n",
              " -0.02929910458624363,\n",
              " 0.0028224694542586803,\n",
              " 0.04217402637004852,\n",
              " 0.017522307112812996,\n",
              " -0.02317088656127453,\n",
              " -0.012836667709052563,\n",
              " 0.016060883179306984,\n",
              " 0.07096286863088608,\n",
              " 0.022300882264971733,\n",
              " -0.030652055516839027,\n",
              " -0.02046220190823078,\n",
              " -0.008860406465828419,\n",
              " 0.044191617518663406,\n",
              " 0.012760547921061516,\n",
              " 0.017109554260969162,\n",
              " 0.00567222386598587,\n",
              " 0.020018693059682846,\n",
              " -0.015990694984793663,\n",
              " -0.03650399297475815,\n",
              " -0.010141105391085148,\n",
              " -0.024074191227555275,\n",
              " 0.03260262683033943,\n",
              " 0.01904658041894436,\n",
              " 0.03513059765100479,\n",
              " -0.012212435714900494,\n",
              " 0.0036663140635937452,\n",
              " -0.0070173488929867744,\n",
              " -0.03424908220767975,\n",
              " 0.036729518324136734,\n",
              " 0.01946347951889038,\n",
              " -0.00860413908958435,\n",
              " -0.04971911385655403,\n",
              " -0.059182457625865936,\n",
              " -0.002848780946806073,\n",
              " 0.031969111412763596,\n",
              " -0.026185661554336548,\n",
              " 0.05550776794552803,\n",
              " -0.004006619565188885,\n",
              " -0.051209691911935806,\n",
              " 0.06815080344676971,\n",
              " -0.05306592956185341,\n",
              " 0.020680462941527367,\n",
              " -0.07201379537582397,\n",
              " -0.01924624852836132,\n",
              " 0.005424595437943935,\n",
              " -0.043159838765859604,\n",
              " -0.004355010110884905,\n",
              " -0.0022963096853345633,\n",
              " 0.028794437646865845,\n",
              " 0.030889587476849556,\n",
              " -0.007004606071859598,\n",
              " 0.07111821323633194,\n",
              " -0.013412009924650192,\n",
              " -0.014901253394782543,\n",
              " 0.014481945894658566,\n",
              " -0.038996364921331406,\n",
              " -0.03584769740700722,\n",
              " -0.0066221775487065315,\n",
              " 0.008168015629053116,\n",
              " -0.03866076096892357,\n",
              " -0.024964667856693268,\n",
              " -0.03399388864636421,\n",
              " 0.07047809660434723,\n",
              " -0.002807113341987133,\n",
              " -0.0026940142270177603,\n",
              " -0.02858911268413067,\n",
              " 0.06441131234169006,\n",
              " 0.022924337536096573,\n",
              " -0.038946185261011124,\n",
              " -0.005655820947140455,\n",
              " -0.05000632628798485,\n",
              " -0.014447260648012161,\n",
              " -0.002026891801506281,\n",
              " 0.0030106124468147755,\n",
              " 0.05433937534689903,\n",
              " 0.0038927753921598196,\n",
              " 0.01058945618569851,\n",
              " -0.020083194598555565,\n",
              " 0.06434295326471329,\n",
              " -0.037731949239969254,\n",
              " 0.019364971667528152,\n",
              " -0.01708812639117241,\n",
              " -0.007275398354977369,\n",
              " 0.006923719309270382,\n",
              " 0.03294694796204567,\n",
              " 0.008821401745080948,\n",
              " -0.035904500633478165,\n",
              " 0.023028215393424034,\n",
              " -0.028629709035158157,\n",
              " -0.012202284298837185,\n",
              " -0.07996783405542374,\n",
              " 0.009951084852218628,\n",
              " -0.018842991441488266,\n",
              " 0.04279767721891403,\n",
              " 0.0063218362629413605,\n",
              " -0.020923594012856483,\n",
              " 0.01167957205325365,\n",
              " 0.02149077132344246,\n",
              " 0.04855334386229515,\n",
              " 0.01931636407971382,\n",
              " -0.00798702146857977,\n",
              " 0.005307495128363371,\n",
              " -0.010870776139199734,\n",
              " 0.02465776912868023,\n",
              " -0.05784374848008156,\n",
              " -0.012009432539343834,\n",
              " -0.0010353594552725554,\n",
              " 0.001357968314550817,\n",
              " 0.03233030438423157,\n",
              " -0.03482642024755478,\n",
              " -0.04115253686904907,\n",
              " -0.0016379851149395108,\n",
              " -0.016736943274736404,\n",
              " 0.03030269965529442,\n",
              " 0.0070823547430336475,\n",
              " 0.013011662289500237,\n",
              " 0.0013094530440866947,\n",
              " -0.023420250043272972,\n",
              " 0.04642616957426071,\n",
              " 0.04938816651701927,\n",
              " 0.011798360385000706,\n",
              " -0.045309584587812424,\n",
              " 0.009469387121498585,\n",
              " -0.005720063112676144,\n",
              " -0.005492646712809801,\n",
              " 0.07672107964754105,\n",
              " 0.05767561495304108,\n",
              " -0.016651729121804237,\n",
              " 0.01272104773670435,\n",
              " -0.028916891664266586,\n",
              " -0.01569833792746067,\n",
              " 0.013438977301120758,\n",
              " -0.04521883279085159,\n",
              " -0.04580063745379448,\n",
              " -0.06875801086425781,\n",
              " -0.008748088032007217,\n",
              " 0.0034978643525391817,\n",
              " -0.05619722977280617,\n",
              " -0.046840690076351166,\n",
              " 0.049902353435754776,\n",
              " 0.03386502340435982,\n",
              " -0.008411991409957409,\n",
              " -0.003451331052929163,\n",
              " -0.02074151486158371,\n",
              " 0.07574094086885452,\n",
              " -0.01795017346739769,\n",
              " 0.015205703675746918,\n",
              " 0.020769039168953896,\n",
              " -0.0004893033183179796,\n",
              " -0.04977618157863617,\n",
              " -0.06083740293979645,\n",
              " -0.011869067326188087,\n",
              " 0.04016096889972687,\n",
              " -0.0056722043082118034,\n",
              " -0.02773195691406727,\n",
              " 0.01636294275522232,\n",
              " 0.01701270416378975,\n",
              " 0.03908716142177582,\n",
              " -0.015088760294020176,\n",
              " -0.03776533156633377,\n",
              " -0.02017400600016117,\n",
              " -0.05718950182199478,\n",
              " -0.04964404180645943,\n",
              " -0.008687540888786316,\n",
              " -0.00674031674861908,\n",
              " -0.009721371345221996,\n",
              " 0.00877001229673624,\n",
              " -0.04894879087805748,\n",
              " 0.029544781893491745,\n",
              " 0.0656760111451149,\n",
              " 0.01786809228360653,\n",
              " 0.03679507598280907,\n",
              " -0.048695698380470276,\n",
              " 0.05084334313869476,\n",
              " -0.0012932618847116828,\n",
              " -0.014733269810676575,\n",
              " -0.0942688137292862,\n",
              " -0.005923083983361721,\n",
              " 0.0548890121281147,\n",
              " -0.0027005928568542004,\n",
              " 0.026992907747626305,\n",
              " -0.00731872720643878,\n",
              " -0.06826119124889374,\n",
              " -0.02900216169655323,\n",
              " 0.004012713208794594,\n",
              " 0.013650557026267052,\n",
              " 0.02704375982284546,\n",
              " 0.03710830584168434,\n",
              " 0.033099979162216187,\n",
              " 0.01301198173314333,\n",
              " -0.05733863264322281,\n",
              " 0.025459110736846924,\n",
              " 0.024094557389616966,\n",
              " -0.007090229541063309,\n",
              " -0.025551943108439445,\n",
              " 0.036184437572956085,\n",
              " 0.038272250443696976,\n",
              " -0.027310671284794807,\n",
              " 0.027453413233160973,\n",
              " 0.038615234196186066,\n",
              " 0.02579406090080738,\n",
              " 0.05250363424420357,\n",
              " 0.022117899730801582,\n",
              " -0.05710281804203987,\n",
              " -0.0017957596573978662,\n",
              " 0.03591448813676834,\n",
              " -0.005846180021762848,\n",
              " -0.06997204571962357,\n",
              " 0.00024452630896121264,\n",
              " -0.010681462474167347,\n",
              " 0.06773526221513748,\n",
              " -0.005376582033932209,\n",
              " 0.022301286458969116,\n",
              " 0.022317348048090935,\n",
              " 0.012877714820206165,\n",
              " -0.04674927517771721,\n",
              " 0.05696335807442665,\n",
              " -0.01296178437769413,\n",
              " 0.016913650557398796,\n",
              " -0.053046829998493195,\n",
              " -0.002701016841456294,\n",
              " 0.003923126962035894,\n",
              " 0.03747446462512016,\n",
              " 0.11272288858890533,\n",
              " -0.0015829706098884344,\n",
              " -0.05584847927093506,\n",
              " 0.09707442671060562,\n",
              " -0.00024738904903642833,\n",
              " -0.03714948520064354,\n",
              " -0.04195793718099594,\n",
              " 0.009514124132692814,\n",
              " 0.019299393519759178,\n",
              " -0.03335732966661453,\n",
              " 0.0021547258365899324,\n",
              " 0.053686920553445816,\n",
              " -0.03058801032602787,\n",
              " -0.0028617610223591328,\n",
              " 0.03269273787736893,\n",
              " 0.02336142770946026,\n",
              " -0.018097469583153725,\n",
              " -0.020934492349624634,\n",
              " 0.03093210980296135,\n",
              " -0.008286625146865845,\n",
              " -0.029237965121865273,\n",
              " -0.03758196532726288,\n",
              " -0.02840360254049301,\n",
              " 0.053243815898895264,\n",
              " 0.010723110288381577,\n",
              " -0.020957577973604202,\n",
              " -0.022098371759057045,\n",
              " 0.06305725127458572,\n",
              " -0.023262832313776016,\n",
              " 0.01595097780227661,\n",
              " 0.00835984107106924,\n",
              " 0.07245082408189774,\n",
              " 0.008686445653438568,\n",
              " 0.0012503870530053973,\n",
              " 7.557651406386867e-05,\n",
              " 0.03862207755446434,\n",
              " -0.019229695200920105,\n",
              " 0.014497706666588783,\n",
              " 0.012569671496748924,\n",
              " -0.026799125596880913,\n",
              " 0.019178958609700203,\n",
              " 0.026653669774532318,\n",
              " -0.014713150449097157,\n",
              " -0.00043338595423847437,\n",
              " 0.08456723392009735,\n",
              " -0.062270551919937134,\n",
              " 0.008901653811335564,\n",
              " -0.0008224630146287382,\n",
              " -0.009016134776175022,\n",
              " 0.010196023620665073,\n",
              " -0.05758286267518997,\n",
              " 0.001213831827044487,\n",
              " -0.012950764037668705,\n",
              " 0.08539506793022156,\n",
              " -0.01374772097915411,\n",
              " 0.03781634569168091,\n",
              " -0.015076442621648312,\n",
              " -0.0145783182233572,\n",
              " -0.012429311871528625,\n",
              " -0.05112070590257645,\n",
              " 0.042674072086811066,\n",
              " -0.03859506547451019,\n",
              " 0.0258342158049345,\n",
              " -0.0033613459672778845,\n",
              " -0.008885742165148258,\n",
              " 0.009936174377799034,\n",
              " 0.008650175295770168,\n",
              " -0.030997151508927345,\n",
              " 0.02414288930594921,\n",
              " 0.019081875681877136,\n",
              " 0.021299712359905243,\n",
              " -0.0016012733103707433,\n",
              " -0.03776213154196739,\n",
              " -0.007063841447234154,\n",
              " -0.010149193927645683,\n",
              " 0.018930433318018913,\n",
              " -0.030712800100445747,\n",
              " -0.0070159053429961205,\n",
              " -0.016985133290290833,\n",
              " 0.07048439234495163,\n",
              " 0.004174362868070602,\n",
              " -0.0022258968092501163,\n",
              " -0.02214689739048481,\n",
              " 0.027761224657297134,\n",
              " 0.034946225583553314,\n",
              " -0.044282216578722,\n",
              " -0.034360796213150024,\n",
              " -0.03490037843585014,\n",
              " 0.018017347902059555,\n",
              " -0.04288153350353241,\n",
              " 0.08776484429836273,\n",
              " 0.036676447838544846,\n",
              " -0.004171433392912149,\n",
              " -0.023275645449757576,\n",
              " -0.07430888712406158,\n",
              " -0.05915144830942154,\n",
              " 0.09232326596975327,\n",
              " -0.025909466668963432,\n",
              " -0.048947375267744064,\n",
              " -0.04248259216547012,\n",
              " -0.008888361044228077,\n",
              " -0.03014891780912876,\n",
              " -0.04645727202296257,\n",
              " -0.012320748530328274,\n",
              " -0.05791699141263962,\n",
              " -0.031029148027300835,\n",
              " -0.008003050461411476,\n",
              " 0.06263463944196701,\n",
              " -0.0003443692403379828,\n",
              " -0.0023389095440506935,\n",
              " -0.024600928649306297,\n",
              " -0.012160244397819042,\n",
              " 0.024075577035546303,\n",
              " 0.04455995932221413,\n",
              " -0.034846968948841095,\n",
              " 0.003009699983522296,\n",
              " -0.006747885141521692,\n",
              " 0.033100686967372894,\n",
              " 0.017554784193634987,\n",
              " 0.0066912914626300335,\n",
              " -0.03815562650561333,\n",
              " 0.03287581354379654,\n",
              " 0.008438421413302422,\n",
              " 0.05949356034398079,\n",
              " 0.05250370502471924,\n",
              " -0.025399159640073776,\n",
              " -0.00804033875465393,\n",
              " -0.0951286181807518,\n",
              " 0.04880988597869873,\n",
              " -0.030327320098876953,\n",
              " -0.00582280196249485,\n",
              " 0.06371273845434189,\n",
              " -0.008379978127777576,\n",
              " -0.01847619190812111,\n",
              " 0.0033510157372802496,\n",
              " -0.06536964327096939,\n",
              " -0.007209788542240858,\n",
              " 0.0060051498003304005,\n",
              " 0.00125016737729311,\n",
              " 0.0346645750105381,\n",
              " -0.0015321788378059864,\n",
              " -0.0008062190026976168,\n",
              " 0.012339606881141663,\n",
              " 0.004700178746134043,\n",
              " -0.014207116328179836,\n",
              " 0.048131342977285385,\n",
              " -0.028085095807909966,\n",
              " 0.0396636538207531,\n",
              " -0.026388678699731827,\n",
              " 0.02602994255721569,\n",
              " 0.005051842425018549,\n",
              " 0.061865344643592834,\n",
              " 0.07961132377386093,\n",
              " -0.03165215626358986,\n",
              " -0.019245469942688942,\n",
              " 0.047423698008060455,\n",
              " 0.029522284865379333,\n",
              " 0.0305622685700655,\n",
              " -0.04864562675356865,\n",
              " -0.05137154087424278,\n",
              " -0.03016076795756817,\n",
              " -0.010218193754553795,\n",
              " 0.02187609300017357,\n",
              " 0.06728222966194153,\n",
              " -0.058545537292957306,\n",
              " -0.07691137492656708,\n",
              " -0.04078034684062004,\n",
              " 0.02178112603724003,\n",
              " 0.004406917840242386,\n",
              " 0.01251312531530857,\n",
              " -0.01934864930808544,\n",
              " -0.00024852779461070895,\n",
              " -0.027734138071537018,\n",
              " 0.044431790709495544,\n",
              " 0.02758478745818138,\n",
              " 0.044154103845357895,\n",
              " -0.03837814927101135,\n",
              " 0.032868776470422745,\n",
              " 0.009355633519589901,\n",
              " -0.007741476874798536,\n",
              " -0.024058016017079353,\n",
              " -0.02246076613664627,\n",
              " -0.004031325690448284,\n",
              " -0.036114875227212906,\n",
              " 0.058158028870821,\n",
              " 0.0031521052587777376,\n",
              " 0.03671975061297417,\n",
              " -0.026520084589719772,\n",
              " -0.06631795316934586,\n",
              " 0.10039965808391571,\n",
              " -0.002299437765032053,\n",
              " -0.011070421896874905,\n",
              " -0.027540046721696854,\n",
              " 0.0407177172601223,\n",
              " 0.06292594969272614,\n",
              " 0.032785814255476,\n",
              " 0.021106448024511337,\n",
              " -0.04404228925704956,\n",
              " -0.005727207753807306,\n",
              " 0.06395310908555984,\n",
              " -0.007708157878369093]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "H6v2VV9lBNlb"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents,\n",
        "    embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4F_AqazDjvc",
        "outputId": "de298b49-a5ba-47f3-d9fa-c82bdbcdace0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_Chroma__ensure_collection',\n",
              " '_Chroma__query_collection',\n",
              " '_LANGCHAIN_DEFAULT_COLLECTION_NAME',\n",
              " '__abstractmethods__',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abc_impl',\n",
              " '_asimilarity_search_with_relevance_scores',\n",
              " '_chroma_collection',\n",
              " '_client',\n",
              " '_client_settings',\n",
              " '_collection',\n",
              " '_collection_metadata',\n",
              " '_collection_name',\n",
              " '_cosine_relevance_score_fn',\n",
              " '_embedding_function',\n",
              " '_euclidean_relevance_score_fn',\n",
              " '_get_retriever_tags',\n",
              " '_max_inner_product_relevance_score_fn',\n",
              " '_persist_directory',\n",
              " '_select_relevance_score_fn',\n",
              " '_similarity_search_with_relevance_scores',\n",
              " 'aadd_documents',\n",
              " 'aadd_texts',\n",
              " 'add_documents',\n",
              " 'add_images',\n",
              " 'add_texts',\n",
              " 'adelete',\n",
              " 'afrom_documents',\n",
              " 'afrom_texts',\n",
              " 'aget_by_ids',\n",
              " 'amax_marginal_relevance_search',\n",
              " 'amax_marginal_relevance_search_by_vector',\n",
              " 'as_retriever',\n",
              " 'asearch',\n",
              " 'asimilarity_search',\n",
              " 'asimilarity_search_by_vector',\n",
              " 'asimilarity_search_with_relevance_scores',\n",
              " 'asimilarity_search_with_score',\n",
              " 'delete',\n",
              " 'delete_collection',\n",
              " 'embeddings',\n",
              " 'encode_image',\n",
              " 'from_documents',\n",
              " 'from_texts',\n",
              " 'get',\n",
              " 'get_by_ids',\n",
              " 'max_marginal_relevance_search',\n",
              " 'max_marginal_relevance_search_by_vector',\n",
              " 'override_relevance_score_fn',\n",
              " 'reset_collection',\n",
              " 'search',\n",
              " 'similarity_search',\n",
              " 'similarity_search_by_image',\n",
              " 'similarity_search_by_image_with_relevance_score',\n",
              " 'similarity_search_by_vector',\n",
              " 'similarity_search_by_vector_with_relevance_scores',\n",
              " 'similarity_search_with_relevance_scores',\n",
              " 'similarity_search_with_score',\n",
              " 'update_document',\n",
              " 'update_documents']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "list(dir(vectorstore))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV6SYPkLCUZf",
        "outputId": "78838bcb-6481-4c17-d8ea-198f2eba9286"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7c1455465a80>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enYsspQlCVTx",
        "outputId": "75a391bb-a281-423e-fbf6-d276c98a74ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
              " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "vectorstore.similarity_search(\"cat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhUxC04ODcB_",
        "outputId": "c6e30280-8a3b-4ff4-e6c0-0c2fc54506c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
              " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "await vectorstore.asimilarity_search(\"cat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqdDbqlqD66I",
        "outputId": "eb70e2bd-d237-411b-e681-f40459dff6e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              "  0.6688324213027954),\n",
              " (Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              "  0.6688324213027954),\n",
              " (Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
              "  0.9940857887268066),\n",
              " (Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
              "  0.9940857887268066)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# Note that providers implement different scores; Chroma here\n",
        "# returns a distance metric that should vary inversely with\n",
        "# similarity.\n",
        "\n",
        "vectorstore.similarity_search_with_score(\"cat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "M5wF724BDNb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dcaf8aa-fc0a-4603-fb35-d220ef0c1628"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
              " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
              " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "embedding = embeddings.embed_query(\"cat\")# convert cat into vector\n",
        "\n",
        "vectorstore.similarity_search_by_vector(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "fVH5rbdBFGiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcc3b57-ed0b-4295-fdc8-11fe4f731011"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.014495342038571835,\n",
              " -0.004356286954134703,\n",
              " 0.030960695818066597,\n",
              " -0.01937987469136715,\n",
              " 0.046561747789382935,\n",
              " 0.07971760630607605,\n",
              " 0.044616784900426865,\n",
              " -0.019115572795271873,\n",
              " 0.08993305265903473,\n",
              " 0.05842595174908638,\n",
              " -0.0453537255525589,\n",
              " 0.018237140029668808,\n",
              " 0.08033973723649979,\n",
              " 0.02752574160695076,\n",
              " -0.028318511322140694,\n",
              " -0.02716362290084362,\n",
              " 0.006191493012011051,\n",
              " 0.005197346676141024,\n",
              " -0.01692069135606289,\n",
              " 0.03503800556063652,\n",
              " 0.08894353359937668,\n",
              " -0.010495638474822044,\n",
              " 0.04114319011569023,\n",
              " -0.01638908125460148,\n",
              " -0.03600003570318222,\n",
              " 0.01174926944077015,\n",
              " 0.03139335289597511,\n",
              " 0.03141077607870102,\n",
              " -0.0031625358387827873,\n",
              " -0.016714120283722878,\n",
              " 0.042587678879499435,\n",
              " 0.04249170795083046,\n",
              " 0.01343710720539093,\n",
              " 0.02279532328248024,\n",
              " 0.0567803755402565,\n",
              " -0.01019603107124567,\n",
              " -0.014729867689311504,\n",
              " 0.06853161752223969,\n",
              " -0.008628561161458492,\n",
              " -0.022934652864933014,\n",
              " -0.03452526405453682,\n",
              " 0.01392278727144003,\n",
              " -0.022488689050078392,\n",
              " 0.02131492830812931,\n",
              " -0.01229527685791254,\n",
              " 0.02273530140519142,\n",
              " 0.026013249531388283,\n",
              " 0.054191142320632935,\n",
              " -0.019599685445427895,\n",
              " 0.028602680191397667,\n",
              " 0.03854745998978615,\n",
              " 0.057732827961444855,\n",
              " -0.011696861125528812,\n",
              " 0.03517353534698486,\n",
              " -0.03327692300081253,\n",
              " -0.023202788084745407,\n",
              " -0.03749627247452736,\n",
              " -0.043892234563827515,\n",
              " 0.027108289301395416,\n",
              " 0.03884704411029816,\n",
              " 0.05957861244678497,\n",
              " 0.03194914013147354,\n",
              " 0.04493435472249985,\n",
              " 0.018000636249780655,\n",
              " 0.0012458269484341145,\n",
              " -0.05043290555477142,\n",
              " -0.007925835438072681,\n",
              " 0.029667330905795097,\n",
              " -0.041334379464387894,\n",
              " 0.04302045702934265,\n",
              " -0.065669484436512,\n",
              " 0.012996182776987553,\n",
              " -0.06907743215560913,\n",
              " 0.06380390375852585,\n",
              " -0.06687374413013458,\n",
              " -0.004804658703505993,\n",
              " 0.03108067996799946,\n",
              " 0.010110396891832352,\n",
              " 0.013045981526374817,\n",
              " -0.0013597144279628992,\n",
              " -0.025310993194580078,\n",
              " -0.016427835449576378,\n",
              " 0.12784525752067566,\n",
              " 0.0562836118042469,\n",
              " 0.03940841555595398,\n",
              " 0.010673423297703266,\n",
              " 0.044212907552719116,\n",
              " 0.048849403858184814,\n",
              " -0.029595036059617996,\n",
              " 0.018391147255897522,\n",
              " 0.032150592654943466,\n",
              " 0.0052758133970201015,\n",
              " 0.036253951489925385,\n",
              " 0.011419572867453098,\n",
              " 0.09523913264274597,\n",
              " -0.016540775075554848,\n",
              " -0.11345330625772476,\n",
              " -0.03300795704126358,\n",
              " -0.009019090794026852,\n",
              " -0.007627598475664854,\n",
              " -0.022728251293301582,\n",
              " 0.008672826923429966,\n",
              " -0.023972366005182266,\n",
              " -0.04063356667757034,\n",
              " 0.05192171782255173,\n",
              " 0.03169471397995949,\n",
              " 0.023015039041638374,\n",
              " 0.018516376614570618,\n",
              " -0.06874707341194153,\n",
              " 0.012852507643401623,\n",
              " 0.018752826377749443,\n",
              " -0.034104835242033005,\n",
              " -0.029502661898732185,\n",
              " 0.01095086894929409,\n",
              " 0.017953800037503242,\n",
              " 0.06737758964300156,\n",
              " -0.04356754571199417,\n",
              " 0.02228250727057457,\n",
              " -0.04604680463671684,\n",
              " -0.01731697842478752,\n",
              " 0.015446624718606472,\n",
              " -0.042186420410871506,\n",
              " -0.032552171498537064,\n",
              " -0.013582481071352959,\n",
              " 0.03246605396270752,\n",
              " -0.016573838889598846,\n",
              " -0.024401187896728516,\n",
              " -0.0072038196958601475,\n",
              " -0.07549452036619186,\n",
              " -0.009203414432704449,\n",
              " 0.02896364964544773,\n",
              " -0.10613583028316498,\n",
              " 0.03781411796808243,\n",
              " 0.040271855890750885,\n",
              " -0.012917520478367805,\n",
              " -0.014856315217912197,\n",
              " -0.002216820605099201,\n",
              " -0.042743101716041565,\n",
              " 0.017111534252762794,\n",
              " 0.005375030916184187,\n",
              " -0.021967323496937752,\n",
              " -0.023669961839914322,\n",
              " -0.05614436790347099,\n",
              " 0.036794159561395645,\n",
              " 0.007754378020763397,\n",
              " 0.013125460594892502,\n",
              " -0.013753363862633705,\n",
              " 0.02151157520711422,\n",
              " 0.02284136228263378,\n",
              " 0.05279908701777458,\n",
              " -0.01927850767970085,\n",
              " -0.040149297565221786,\n",
              " -0.029326194897294044,\n",
              " -0.0826430469751358,\n",
              " -0.005802650470286608,\n",
              " 0.03755241632461548,\n",
              " 0.056654009968042374,\n",
              " -0.046860672533512115,\n",
              " 0.012279239483177662,\n",
              " -0.012804095633327961,\n",
              " -0.008213716559112072,\n",
              " 0.015394221991300583,\n",
              " -0.019039791077375412,\n",
              " 0.0004277710395399481,\n",
              " -0.03749733045697212,\n",
              " 0.004896875936537981,\n",
              " 0.006666247732937336,\n",
              " -0.0368269719183445,\n",
              " 0.006604892201721668,\n",
              " -0.041655901819467545,\n",
              " -0.015426828525960445,\n",
              " -0.026914777234196663,\n",
              " -0.033153023570775986,\n",
              " -0.006163540296256542,\n",
              " -0.043462660163640976,\n",
              " 0.03373173996806145,\n",
              " -0.022072628140449524,\n",
              " -0.059182554483413696,\n",
              " -0.04107022285461426,\n",
              " -0.011897679418325424,\n",
              " 0.0894283875823021,\n",
              " -0.010708942078053951,\n",
              " 0.0030413600616157055,\n",
              " -0.05152417719364166,\n",
              " -0.021752290427684784,\n",
              " 0.0021489188075065613,\n",
              " -0.032366637140512466,\n",
              " 0.04301592707633972,\n",
              " -0.014634218066930771,\n",
              " 0.004646045155823231,\n",
              " -0.03228824958205223,\n",
              " -0.030899638310074806,\n",
              " 0.034567225724458694,\n",
              " -0.029848938807845116,\n",
              " 0.007353954948484898,\n",
              " 0.029665948823094368,\n",
              " 0.05472755432128906,\n",
              " -0.056341275572776794,\n",
              " 0.02834026888012886,\n",
              " 0.01007724180817604,\n",
              " 0.07145177572965622,\n",
              " 0.014313459396362305,\n",
              " 0.03674114868044853,\n",
              " -0.03532882407307625,\n",
              " -0.03655190020799637,\n",
              " 0.004517338238656521,\n",
              " -0.02057579904794693,\n",
              " -0.04629502817988396,\n",
              " 0.04579285904765129,\n",
              " 0.02886699140071869,\n",
              " -0.0479821041226387,\n",
              " 0.02997475303709507,\n",
              " -0.025145582854747772,\n",
              " -0.024457167834043503,\n",
              " -0.044499240815639496,\n",
              " -0.013999933376908302,\n",
              " 0.017638012766838074,\n",
              " -0.005100300069898367,\n",
              " 0.050823409110307693,\n",
              " -0.017096595838665962,\n",
              " 0.028085004538297653,\n",
              " 0.007485541515052319,\n",
              " 0.04336225986480713,\n",
              " -0.023835768923163414,\n",
              " 0.05174798145890236,\n",
              " -0.0036513807717710733,\n",
              " -0.03772061690688133,\n",
              " -0.009190001524984837,\n",
              " 0.05010225996375084,\n",
              " -0.028060661628842354,\n",
              " -0.005852084141224623,\n",
              " 0.0586034432053566,\n",
              " 0.02624264545738697,\n",
              " -0.03800287842750549,\n",
              " -0.0008826943812891841,\n",
              " 0.022417888045310974,\n",
              " -0.004312317818403244,\n",
              " -0.06868863105773926,\n",
              " 0.032671067863702774,\n",
              " -0.0022994743194431067,\n",
              " 0.0007701306021772325,\n",
              " -0.023832449689507484,\n",
              " 0.03522908315062523,\n",
              " 0.01721302978694439,\n",
              " 0.03291720896959305,\n",
              " 0.011703910306096077,\n",
              " 0.016229702159762383,\n",
              " 0.11202928423881531,\n",
              " -0.012567290104925632,\n",
              " -0.028998728841543198,\n",
              " -0.01796942949295044,\n",
              " -0.007563501130789518,\n",
              " 0.019764240831136703,\n",
              " -0.03755462169647217,\n",
              " -0.007266667205840349,\n",
              " -0.033096350729465485,\n",
              " -0.0011382579104974866,\n",
              " 0.01661369390785694,\n",
              " 0.01625390723347664,\n",
              " -0.0527925081551075,\n",
              " -0.0005613145767711103,\n",
              " 0.014885400421917439,\n",
              " -0.05623757094144821,\n",
              " -0.06841340661048889,\n",
              " -0.04774221405386925,\n",
              " -0.09854623675346375,\n",
              " 0.050990991294384,\n",
              " -0.01567322388291359,\n",
              " 0.05277993902564049,\n",
              " -0.03459024429321289,\n",
              " 0.01863996870815754,\n",
              " 0.015093546360731125,\n",
              " -0.05867292732000351,\n",
              " 0.01694985292851925,\n",
              " -0.03468502312898636,\n",
              " 0.03431730717420578,\n",
              " 0.01681828685104847,\n",
              " 0.009453254751861095,\n",
              " -0.0314871110022068,\n",
              " -0.03418149799108505,\n",
              " 0.02665816619992256,\n",
              " -0.020313439890742302,\n",
              " 0.029595443978905678,\n",
              " 0.015466163866221905,\n",
              " -0.0043038055300712585,\n",
              " -0.09575702995061874,\n",
              " -0.01698094606399536,\n",
              " -0.02044876478612423,\n",
              " -0.07961999624967575,\n",
              " 0.02469290792942047,\n",
              " -0.02415757067501545,\n",
              " -0.012890279293060303,\n",
              " -0.0005242861807346344,\n",
              " -0.07397239655256271,\n",
              " 0.026003509759902954,\n",
              " 0.01713055931031704,\n",
              " 0.01858633942902088,\n",
              " 0.02493404597043991,\n",
              " 0.007078996859490871,\n",
              " -0.042431220412254333,\n",
              " 0.03442830592393875,\n",
              " 0.03028200753033161,\n",
              " -0.04095824062824249,\n",
              " -0.018567312508821487,\n",
              " -0.04660164564847946,\n",
              " -0.014636268839240074,\n",
              " 0.06386875361204147,\n",
              " -6.676342309219763e-05,\n",
              " -0.06168491020798683,\n",
              " 0.029985416680574417,\n",
              " 0.04212134703993797,\n",
              " 0.04766783118247986,\n",
              " -0.023553142324090004,\n",
              " 0.012756011448800564,\n",
              " -0.006004644092172384,\n",
              " 0.024423105642199516,\n",
              " -0.12709251046180725,\n",
              " 0.0053990245796740055,\n",
              " -0.018125269562005997,\n",
              " -0.004389484878629446,\n",
              " 0.007763621862977743,\n",
              " -0.001605904079042375,\n",
              " -0.07371946424245834,\n",
              " 0.026210224255919456,\n",
              " 0.015298396348953247,\n",
              " -0.030079368501901627,\n",
              " 0.03138217329978943,\n",
              " 0.0016951576108112931,\n",
              " -0.0025201758835464716,\n",
              " 0.028197355568408966,\n",
              " 0.03749537467956543,\n",
              " 0.033992502838373184,\n",
              " -0.043073005974292755,\n",
              " -0.031338341534137726,\n",
              " -0.010517865419387817,\n",
              " -0.009557870216667652,\n",
              " -0.004246994853019714,\n",
              " 0.019957145676016808,\n",
              " 0.061667490750551224,\n",
              " -0.03625188022851944,\n",
              " 0.043662674725055695,\n",
              " 0.045630957931280136,\n",
              " -0.027949176728725433,\n",
              " 0.03632242605090141,\n",
              " -0.04164774343371391,\n",
              " -0.05566929280757904,\n",
              " -0.014871735125780106,\n",
              " -0.009267404675483704,\n",
              " 0.0297231525182724,\n",
              " -0.03755731135606766,\n",
              " -0.002444678219035268,\n",
              " 0.05032859742641449,\n",
              " 0.06653160601854324,\n",
              " 0.03581548482179642,\n",
              " 0.02291916310787201,\n",
              " 0.007579154334962368,\n",
              " 0.052164290100336075,\n",
              " 0.027341725304722786,\n",
              " 0.029429402202367783,\n",
              " -0.05082295089960098,\n",
              " 0.00852219294756651,\n",
              " 0.03701968491077423,\n",
              " -0.021862531080842018,\n",
              " 0.0066058156080543995,\n",
              " 0.010817384347319603,\n",
              " -0.013875287026166916,\n",
              " 0.03772403672337532,\n",
              " 0.01746552623808384,\n",
              " -0.04079204797744751,\n",
              " -0.017093703150749207,\n",
              " 0.02326754666864872,\n",
              " 0.03330978378653526,\n",
              " -0.061781056225299835,\n",
              " -0.056202881038188934,\n",
              " 0.03451331704854965,\n",
              " 0.0193772092461586,\n",
              " 0.026546454057097435,\n",
              " -0.010458791628479958,\n",
              " 0.010780589655041695,\n",
              " -0.06542893499135971,\n",
              " -0.049370113760232925,\n",
              " -0.025627417489886284,\n",
              " -0.03826787695288658,\n",
              " 0.03643171489238739,\n",
              " -0.011802082881331444,\n",
              " 0.025972934439778328,\n",
              " -0.013640172779560089,\n",
              " -0.01863125152885914,\n",
              " -0.005353703163564205,\n",
              " 0.048920925706624985,\n",
              " -0.0036599019076675177,\n",
              " 0.07777836918830872,\n",
              " -0.007352809887379408,\n",
              " 0.0941699743270874,\n",
              " 0.004023346118628979,\n",
              " 0.04038522019982338,\n",
              " 0.0062688011676073074,\n",
              " 0.017891239374876022,\n",
              " -0.00045396244968287647,\n",
              " -0.005121137946844101,\n",
              " 0.01605778932571411,\n",
              " -0.02151099033653736,\n",
              " 0.05888630449771881,\n",
              " -0.03158915415406227,\n",
              " -0.02463885210454464,\n",
              " -0.060618676245212555,\n",
              " 0.08029492199420929,\n",
              " 0.015040358528494835,\n",
              " 0.023191051557660103,\n",
              " 0.013043473474681377,\n",
              " -0.008983276784420013,\n",
              " -0.01960049197077751,\n",
              " 0.003931979648768902,\n",
              " -0.004162451718002558,\n",
              " -0.0558016262948513,\n",
              " -0.02321023680269718,\n",
              " -0.008982245810329914,\n",
              " -0.03250500187277794,\n",
              " 0.013700757175683975,\n",
              " -0.020671239122748375,\n",
              " -0.01966019533574581,\n",
              " -0.026049116626381874,\n",
              " 0.023425033316016197,\n",
              " -0.0571289137005806,\n",
              " 0.0676134005188942,\n",
              " -0.03147856891155243,\n",
              " 0.004783484153449535,\n",
              " -0.02105563133955002,\n",
              " 0.023404225707054138,\n",
              " 0.009986902587115765,\n",
              " 0.03884095698595047,\n",
              " 0.052581023424863815,\n",
              " -0.051760319620370865,\n",
              " -0.07683192938566208,\n",
              " 0.044021718204021454,\n",
              " -0.01597711443901062,\n",
              " -0.031912315636873245,\n",
              " 0.0016009911196306348,\n",
              " 0.01622863858938217,\n",
              " 0.044367995113134384,\n",
              " -0.05448080226778984,\n",
              " -0.007903790101408958,\n",
              " 0.0320221483707428,\n",
              " 0.04898742586374283,\n",
              " -0.0075828200206160545,\n",
              " -0.03480053320527077,\n",
              " 0.0369291678071022,\n",
              " 0.011668612249195576,\n",
              " 0.024968495592474937,\n",
              " -0.06261155754327774,\n",
              " -0.015086617320775986,\n",
              " -0.010135887190699577,\n",
              " 0.03757711499929428,\n",
              " -0.022884149104356766,\n",
              " -0.024589966982603073,\n",
              " 0.001826342660933733,\n",
              " -0.038321904838085175,\n",
              " 0.04672980308532715,\n",
              " -0.003363755065947771,\n",
              " 0.0018584695644676685,\n",
              " -0.03724170848727226,\n",
              " -0.01473590824753046,\n",
              " 0.07581968605518341,\n",
              " 0.0446288138628006,\n",
              " 0.012080879881978035,\n",
              " -0.0015126736834645271,\n",
              " -0.034375134855508804,\n",
              " 0.020515387877821922,\n",
              " -0.025188425555825233,\n",
              " 0.033369239419698715,\n",
              " -0.020310256630182266,\n",
              " 0.03177556395530701,\n",
              " -0.0025869293604046106,\n",
              " -0.0023815983440726995,\n",
              " -0.06960627436637878,\n",
              " 0.016432374715805054,\n",
              " -0.03503098711371422,\n",
              " -0.007198856677860022,\n",
              " 0.018458737060427666,\n",
              " 0.012718822807073593,\n",
              " -0.0018746207933872938,\n",
              " 0.026161249727010727,\n",
              " -0.016911398619413376,\n",
              " -0.012264116667211056,\n",
              " 0.009560879319906235,\n",
              " -0.02717251516878605,\n",
              " -0.04450945183634758,\n",
              " -0.005724712274968624,\n",
              " 0.00856366939842701,\n",
              " -0.004699807148426771,\n",
              " 0.02714575082063675,\n",
              " 0.03769371286034584,\n",
              " -0.014445687644183636,\n",
              " -0.05889250338077545,\n",
              " -0.027744397521018982,\n",
              " -0.016285041347146034,\n",
              " 0.03391817584633827,\n",
              " 0.04765716567635536,\n",
              " -0.03588756173849106,\n",
              " -0.009045683778822422,\n",
              " 0.06427288055419922,\n",
              " 0.008041891269385815,\n",
              " -0.012796709313988686,\n",
              " 0.0059231845661997795,\n",
              " -0.03066275268793106,\n",
              " -0.019198721274733543,\n",
              " 0.0025056852027773857,\n",
              " -0.011243964545428753,\n",
              " 0.015078474767506123,\n",
              " 0.00023229068028740585,\n",
              " 0.03704901412129402,\n",
              " -0.03782195597887039,\n",
              " 0.03684953600168228,\n",
              " -0.0008073574863374233,\n",
              " -0.0007802497711963952,\n",
              " 0.0031752989161759615,\n",
              " -0.05408695712685585,\n",
              " 0.013624279759824276,\n",
              " -0.03340109437704086,\n",
              " 0.008810549974441528,\n",
              " -0.06255199760198593,\n",
              " 0.0035539439413696527,\n",
              " 0.04046516865491867,\n",
              " -0.02408730424940586,\n",
              " -0.004444975405931473,\n",
              " -0.03216514363884926,\n",
              " 0.042588748037815094,\n",
              " 0.03194030374288559,\n",
              " 0.046359553933143616,\n",
              " -0.011274408549070358,\n",
              " -0.015195063315331936,\n",
              " 0.0842229500412941,\n",
              " 0.04184433072805405,\n",
              " 0.029512617737054825,\n",
              " -0.001792022492736578,\n",
              " 0.017622148618102074,\n",
              " 0.0037141661159694195,\n",
              " -0.04364081472158432,\n",
              " -0.0154451048001647,\n",
              " 0.014997771941125393,\n",
              " 0.02684319205582142,\n",
              " -0.01711355708539486,\n",
              " 0.026975281536579132,\n",
              " 0.0010606914293020964,\n",
              " -0.02851109765470028,\n",
              " 0.013734688982367516,\n",
              " 0.023280879482626915,\n",
              " -0.00656582647934556,\n",
              " -0.005111082922667265,\n",
              " 0.06002664566040039,\n",
              " -0.017135845497250557,\n",
              " -0.04625414311885834,\n",
              " -0.04910166561603546,\n",
              " -0.014541546814143658,\n",
              " -0.04936428740620613,\n",
              " 0.0008820245275273919,\n",
              " 0.010460282675921917,\n",
              " -0.02102917619049549,\n",
              " -0.02600373886525631,\n",
              " 0.06381511688232422,\n",
              " 0.0018244379898533225,\n",
              " -0.015527045354247093,\n",
              " 0.006495023611932993,\n",
              " 0.017078131437301636,\n",
              " 0.021536266431212425,\n",
              " 0.0223929975181818,\n",
              " -0.010455809533596039,\n",
              " 0.047650277614593506,\n",
              " -0.0423312745988369,\n",
              " -0.035175591707229614,\n",
              " 0.03393815457820892,\n",
              " -0.0018116110004484653,\n",
              " -0.018686875700950623,\n",
              " 0.003949502017349005,\n",
              " 0.047485873103141785,\n",
              " 0.040419384837150574,\n",
              " -0.04728281870484352,\n",
              " -0.010295979678630829,\n",
              " 0.014262603595852852,\n",
              " 0.004259874112904072,\n",
              " -0.030744392424821854,\n",
              " 0.013242265209555626,\n",
              " 0.0467415377497673,\n",
              " -0.031172893941402435,\n",
              " -0.06415993720293045,\n",
              " 0.006083221640437841,\n",
              " -0.025612847879529,\n",
              " -0.017075881361961365,\n",
              " 0.004445577971637249,\n",
              " -0.017920581623911858,\n",
              " -0.04836912825703621,\n",
              " 0.023618679493665695,\n",
              " 0.013154268264770508,\n",
              " -0.06432593613862991,\n",
              " 0.02045198157429695,\n",
              " -0.06145953759551048,\n",
              " 0.0196923166513443,\n",
              " 0.00812553521245718,\n",
              " 0.05251162871718407,\n",
              " 0.03422452136874199,\n",
              " -0.023353496566414833,\n",
              " -0.05157572776079178,\n",
              " 0.01932789385318756,\n",
              " 0.0206427164375782,\n",
              " -0.00753538403660059,\n",
              " -0.004960597492754459,\n",
              " -0.01940508559346199,\n",
              " -0.048446159809827805,\n",
              " 0.01647992804646492,\n",
              " 0.009138318710029125,\n",
              " -0.046364326030015945,\n",
              " -0.006677676923573017,\n",
              " -0.03899603709578514,\n",
              " -0.04417090490460396,\n",
              " -0.04246145114302635,\n",
              " 0.0019707854371517897,\n",
              " -0.0732458084821701,\n",
              " -0.028470469638705254,\n",
              " 0.06270482391119003,\n",
              " -0.007610340137034655,\n",
              " -0.020705873146653175,\n",
              " -0.007499265018850565,\n",
              " 0.04953429847955704,\n",
              " -0.05480872839689255,\n",
              " -0.07341049611568451,\n",
              " 0.0029613978695124388,\n",
              " -0.00868128053843975,\n",
              " -0.013636158779263496,\n",
              " -0.006363636814057827,\n",
              " -0.0295990202575922,\n",
              " -0.04738885164260864,\n",
              " 0.024345602840185165,\n",
              " 0.02689945511519909,\n",
              " 0.060283202677965164,\n",
              " -0.05194453150033951,\n",
              " -0.00316982576623559,\n",
              " 0.023382510989904404,\n",
              " -0.017933974042534828,\n",
              " -0.025890206918120384,\n",
              " -0.04840443655848503,\n",
              " -0.015405340120196342,\n",
              " 0.04982003942131996,\n",
              " 0.008434398099780083,\n",
              " 0.061632975935935974,\n",
              " -0.043821316212415695,\n",
              " 0.01619947887957096,\n",
              " 0.027641762048006058,\n",
              " 0.0005397460190579295,\n",
              " -0.006916711572557688,\n",
              " 0.03270827233791351,\n",
              " -0.006939973682165146,\n",
              " 0.007771560922265053,\n",
              " -0.03916696086525917,\n",
              " -0.013202338479459286,\n",
              " -0.05303141847252846,\n",
              " 0.00993372779339552,\n",
              " -0.03915252164006233,\n",
              " -0.023788580670952797,\n",
              " 0.004321413114666939,\n",
              " 0.004640771541744471,\n",
              " -0.04862729832530022,\n",
              " -0.043637678027153015,\n",
              " -0.01829393208026886,\n",
              " 0.018183691427111626,\n",
              " -0.01472838968038559,\n",
              " -0.026670178398489952,\n",
              " -0.0344395712018013,\n",
              " 0.032019827514886856,\n",
              " 0.01825467124581337,\n",
              " -0.04060547426342964,\n",
              " 0.016212722286581993,\n",
              " 0.05365429073572159,\n",
              " 0.0398184135556221,\n",
              " 0.04877550154924393,\n",
              " -0.05641520023345947,\n",
              " 0.020684603601694107,\n",
              " -0.00805574469268322,\n",
              " -0.03198995441198349,\n",
              " -0.04015467315912247,\n",
              " 0.032035645097494125,\n",
              " -7.323707541218027e-05,\n",
              " -0.06813272833824158,\n",
              " 0.04268966615200043,\n",
              " 0.035583265125751495,\n",
              " 0.0023574193473905325,\n",
              " 0.013166993856430054,\n",
              " 0.034896120429039,\n",
              " -0.011006330139935017,\n",
              " -0.051365114748477936,\n",
              " -0.008262085728347301,\n",
              " -0.010505313985049725,\n",
              " 0.011138434521853924,\n",
              " -0.04577609524130821,\n",
              " 0.03484855964779854,\n",
              " -0.008504487574100494,\n",
              " -0.003009496256709099,\n",
              " -0.046396855264902115,\n",
              " 0.0727643296122551,\n",
              " -0.056962523609399796,\n",
              " 0.023479048162698746,\n",
              " -0.021565020084381104,\n",
              " -0.008273879997432232,\n",
              " -0.05476883798837662,\n",
              " -0.07189113646745682,\n",
              " 0.05023609474301338,\n",
              " -0.0134401461109519,\n",
              " 0.007842227816581726,\n",
              " 0.020003508776426315,\n",
              " -0.005303300451487303,\n",
              " 0.0071795242838561535,\n",
              " -0.02265780419111252,\n",
              " 0.06328634172677994,\n",
              " 0.01012442260980606,\n",
              " 0.032010436058044434,\n",
              " -0.013737544417381287,\n",
              " 0.010469690896570683,\n",
              " -0.0047478205524384975,\n",
              " 0.06512557715177536,\n",
              " 0.006134385708719492,\n",
              " -0.006611072923988104,\n",
              " -0.008178671821951866,\n",
              " -0.02132396586239338,\n",
              " 0.03307435289025307,\n",
              " 0.05339197814464569,\n",
              " 0.08201231807470322,\n",
              " 0.04520352929830551,\n",
              " -0.026250004768371582,\n",
              " 0.028347637504339218,\n",
              " 0.012580775655806065,\n",
              " 0.12949956953525543,\n",
              " 0.05566151440143585,\n",
              " 0.023507010191679,\n",
              " 0.028837144374847412,\n",
              " 0.05038142204284668,\n",
              " -0.033684078603982925,\n",
              " 0.034963689744472504,\n",
              " 0.006386245135217905,\n",
              " -0.03819866105914116,\n",
              " 0.011994732543826103,\n",
              " 0.007296890486031771,\n",
              " -0.013642748817801476,\n",
              " 0.024220913648605347,\n",
              " 0.014407324604690075,\n",
              " 0.019176321104168892,\n",
              " -0.013349624350667,\n",
              " 0.04442369565367699,\n",
              " -0.014851129613816738,\n",
              " -0.0032214985694736242,\n",
              " -0.00157309474889189,\n",
              " -0.03889244794845581,\n",
              " -0.015231824479997158,\n",
              " 0.08817324042320251,\n",
              " 0.013628846034407616,\n",
              " -0.02279551513493061,\n",
              " 0.045421943068504333,\n",
              " -0.015625517815351486,\n",
              " 0.047574929893016815,\n",
              " -0.002126862993463874,\n",
              " 0.06680796295404434,\n",
              " 0.034890275448560715,\n",
              " -0.01490753423422575,\n",
              " 0.0739324614405632,\n",
              " 0.011011652648448944,\n",
              " 0.04814859852194786,\n",
              " -0.06317641586065292,\n",
              " 0.0067360843531787395,\n",
              " -0.04438389465212822,\n",
              " 0.04865564405918121]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJBBUPFvFkym"
      },
      "source": [
        "# Retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ndnSdfyoEsfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff0ecff-4ae2-4c4d-cb45-404faeab908a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)  # select top result\n",
        "\n",
        "retriever.batch([\"cat\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "B5Qg_8mtGExp"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                             api_key = userdata.get('GOOGLE_API_KEY')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "vEIe1ORnGv5P"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "message = \"\"\"\n",
        "Answer this question using the provided context only.\n",
        "\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "rKGDcuBQI63Z"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG\n"
      ],
      "metadata": {
        "id": "u-MuFw_svA1V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Sc2yWh5BJJuR"
      },
      "outputs": [],
      "source": [
        "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs3oaRLCJHDp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiJt_KJSJHBf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MzEXYLGWvmn2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFCGSNa3Jn5j",
        "outputId": "36fe1158-ff15-47cd-ff2b-339939e6547d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided text does not contain any information about sharks.\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke(\"tell me about shark\")\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv5FmiyvL0x_"
      },
      "source": [
        "# Now use google gemini embedding model for retriver\n",
        "https://python.langchain.com/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html#langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os-OJ_8ySpgr"
      },
      "source": [
        "# Face detection with embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QeQQ9ohuUMqS"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EeWUwMq_ONLJ",
        "outputId": "a4d8655b-ea82-4040-dbaf-7ba9199466dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "facenet-pytorch 2.6.0 requires Pillow<10.3.0,>=10.2.0, but you have pillow 11.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uq pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VLF-hrWBKmod"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "22HAeuDqTolW",
        "outputId": "87dba5da-4741-497d-b27f-2806e87c1c6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InceptionResnetV1(\n",
              "  (conv2d_1a): BasicConv2d(\n",
              "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (conv2d_2a): BasicConv2d(\n",
              "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (conv2d_2b): BasicConv2d(\n",
              "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (maxpool_3a): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2d_3b): BasicConv2d(\n",
              "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (conv2d_4a): BasicConv2d(\n",
              "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (conv2d_4b): BasicConv2d(\n",
              "    (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "    (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (repeat_1): Sequential(\n",
              "    (0): Block35(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (branch2): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (1): Block35(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (branch2): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (2): Block35(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (branch2): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (3): Block35(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (branch2): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (4): Block35(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (branch2): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (mixed_6a): Mixed_6a(\n",
              "    (branch0): BasicConv2d(\n",
              "      (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (branch1): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (2): BasicConv2d(\n",
              "        (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (branch2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (repeat_2): Sequential(\n",
              "    (0): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (1): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (2): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (3): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (4): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (5): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (6): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (7): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (8): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (9): Block17(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(256, 896, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (mixed_7a): Mixed_7a(\n",
              "    (branch0): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(256, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (branch1): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (2): BasicConv2d(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (branch3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (repeat_3): Sequential(\n",
              "    (0): Block8(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (1): Block8(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (2): Block8(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (3): Block8(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (4): Block8(\n",
              "      (branch0): BasicConv2d(\n",
              "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (branch1): Sequential(\n",
              "        (0): BasicConv2d(\n",
              "          (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (block8): Block8(\n",
              "    (branch0): BasicConv2d(\n",
              "      (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (branch1): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(1792, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(192, 192, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (2): BasicConv2d(\n",
              "        (conv): Conv2d(192, 192, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (conv2d): Conv2d(384, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (avgpool_1a): AdaptiveAvgPool2d(output_size=1)\n",
              "  (dropout): Dropout(p=0.6, inplace=False)\n",
              "  (last_linear): Linear(in_features=1792, out_features=512, bias=False)\n",
              "  (last_bn): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (logits): Linear(in_features=512, out_features=8631, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YtY6raX6Vrsp"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function to transform the image into a tensor\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return preprocess(image).unsqueeze(0)\n",
        "\n",
        "# Function to create image embeddings\n",
        "def create_image_embedding(image_path):\n",
        "    try:\n",
        "        input_tensor = preprocess_image(image_path)\n",
        "        with torch.no_grad():\n",
        "            embeddings = model(input_tensor)# ebedding important line\n",
        "        return embeddings.squeeze().numpy()\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_0AcRHbRVslq",
        "outputId": "9bfe5376-43aa-4a07-b6b8-f369f51be5d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory images: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dhtkU-0lV1-O"
      },
      "outputs": [],
      "source": [
        "# prompt: create python function where we provide image url and imag_name then it save in images folder\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def save_image_from_url(image_url, image_name):\n",
        "  \"\"\"\n",
        "  Downloads an image from a URL and saves it to the 'images' folder.\n",
        "\n",
        "  Args:\n",
        "    image_url: The URL of the image to download.\n",
        "    image_name: The name of the file to save the image as.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(\"images\"):\n",
        "      os.makedirs(\"images\")\n",
        "\n",
        "    image_path = os.path.join(\"images\", image_name)\n",
        "\n",
        "    response = requests.get(image_url, stream=True)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    with open(image_path, 'wb') as file:\n",
        "      for chunk in response.iter_content(chunk_size=8192):\n",
        "        file.write(chunk)\n",
        "\n",
        "    print(f\"Image saved to: {image_path}\")\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading image: {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "01gSmY8gX8ep",
        "outputId": "2137152c-3633-49b2-c0a3-0ed7ed8cf527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image saved to: images/s1.jpg\n",
            "Image saved to: images/q1.jpg\n",
            "Image saved to: images/z1.jpg\n",
            "Image saved to: images/z2.jpg\n",
            "Image saved to: images/s2.jpg\n",
            "Image saved to: images/q2.jpg\n"
          ]
        }
      ],
      "source": [
        "save_image_from_url(\"https://media.licdn.com/dms/image/v2/D4E03AQEEn9DuNlQwvw/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1664654245747?e=2147483647&v=beta&t=NGB0a9aqsgdyxpbuO3rqws95ogJnL_6aRtBDS7IWPfw\",\"s1.jpg\")\n",
        "save_image_from_url(\"https://avatars.githubusercontent.com/u/10209765?v=4\", \"q1.jpg\")\n",
        "\n",
        "save_image_from_url(\"https://media.licdn.com/dms/image/v2/D4D22AQFmuEiR8ttUmw/feedshare-shrink_800/feedshare-shrink_800/0/1711203894556?e=2147483647&v=beta&t=GEZGp_cdogNJCJIGidoEw_DjW2FXZcG4nUUlaNF1Zlc\",\"z1.jpg\")\n",
        "save_image_from_url(\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQBBiqefc7Le97Rn0udVVBkur7RlU53FcQh1A&s\",'z2.jpg')\n",
        "save_image_from_url(\"https://scontent.fkhi4-4.fna.fbcdn.net/v/t39.30808-6/468785380_10160566910882765_300507882801991935_n.jpg?_nc_cat=103&ccb=1-7&_nc_sid=6ee11a&_nc_eui2=AeEk77SJKagGymTo3ibNnnx9YsjCm8DJ0lRiyMKbwMnSVMJqs7YWsJDuzKzXyLHLoFk&_nc_ohc=QJMm9K-AE4QQ7kNvgFE0N2o&_nc_oc=Adi1r8eogMcuDIMMLJvliCOnaaXQ2KnUbbJvY94aAnfInkDB-fyB_1ZXBpDQnWTkZnY&_nc_zt=23&_nc_ht=scontent.fkhi4-4.fna&_nc_gid=AcSynbwc6ukNTxWnUzjhtEe&oh=00_AYCvnF-vj63T-X69PZgBk6JvVjepzybVukgHPSM_6BXkGQ&oe=678003A4\",'s2.jpg')\n",
        "save_image_from_url(\"https://i.ytimg.com/vi/7QD3GKvSyMk/hqdefault.jpg?sqp=-oaymwEmCOADEOgC8quKqQMa8AEB-AHOBYAC0AWKAgwIABABGGUgXChPMA8=&rs=AOn4CLB2EaZsLrClGHqUMUhApQ_sxAcF7Q\",\"q2.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CZJON-xwV-bl",
        "outputId": "09721b69-7af5-4602-93d9-8912c66d9869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading image: No connection adapters were found for 'data:image/jpeg;base64,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'\n"
          ]
        }
      ],
      "source": [
        "save_image_from_url(\"data:image/jpeg;base64,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\",\"q2.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZtpE6lWvW3gD",
        "outputId": "a9490448-f367-484a-9539-0b529459cdba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Embedding Shape: (512,)\n",
            "Image Embedding: [ 0.00058278 -0.01463297 -0.10330155  0.03984535  0.08525636  0.08029363\n",
            " -0.01011138  0.04867227 -0.00259888 -0.0077198  -0.0331905   0.06225294\n",
            "  0.01708761 -0.00982595 -0.01638006  0.00054205  0.04426373  0.00519726\n",
            "  0.04448376 -0.08979201 -0.06033407 -0.00162232  0.09186839  0.03262272\n",
            "  0.06873461  0.02532319  0.03512866 -0.05809444  0.00151727  0.05172818\n",
            " -0.04107905 -0.03755797 -0.04134395 -0.00761167 -0.00674731  0.04443471\n",
            "  0.00570058 -0.02845279 -0.12002273  0.05528227  0.00132485  0.00437373\n",
            " -0.02495593  0.01842853 -0.00357915  0.01889837  0.02980521  0.10357354\n",
            " -0.11238679 -0.02633037  0.02794239 -0.0152881   0.04015452 -0.0039458\n",
            " -0.09202526  0.0715652  -0.03382103  0.07384298  0.01921727 -0.04263638\n",
            "  0.02758526  0.03841295 -0.04435986 -0.05347932 -0.04511738  0.03700717\n",
            " -0.00194125 -0.00450149  0.03157888  0.02796174  0.02416223  0.01202647\n",
            " -0.00905218  0.00880154  0.0189873  -0.01363976 -0.0499587  -0.01557556\n",
            "  0.02297474  0.0651246   0.0614766   0.02845725  0.00367925  0.03151315\n",
            " -0.00906348  0.04034334  0.00219301  0.05856943 -0.00909011 -0.01423355\n",
            "  0.01797742  0.01435157  0.05880354 -0.04344825  0.10147727 -0.04386036\n",
            " -0.01284018  0.02154127 -0.06004561  0.05457702  0.04566431 -0.01077684\n",
            "  0.00699712  0.02765366  0.00357566  0.05447806 -0.01310005 -0.02255299\n",
            "  0.01432354 -0.07601233  0.04908508  0.00389152 -0.06607237 -0.08058616\n",
            "  0.0263547  -0.02808471  0.0695286  -0.00038021 -0.03878854  0.02984364\n",
            " -0.00256646  0.02118442  0.00133815 -0.0472773  -0.01195588 -0.0669666\n",
            " -0.0071366   0.04453232  0.00111025 -0.02842125 -0.0175714  -0.00643478\n",
            "  0.04876323 -0.06472842 -0.09787413  0.00334867  0.00187649 -0.00024767\n",
            "  0.06938788  0.10385709 -0.02833053 -0.00792771 -0.00623726  0.03481311\n",
            "  0.07992636 -0.05045125 -0.00808156  0.00769071  0.05459293 -0.06051681\n",
            "  0.02623133 -0.04237332 -0.01461612  0.07601271  0.02316507 -0.06920444\n",
            "  0.05341151  0.0485434   0.03503026 -0.02546098  0.07508431  0.04377884\n",
            " -0.02852262  0.02682048  0.09139965  0.04641375 -0.05197915 -0.02836282\n",
            " -0.06560951  0.04164857 -0.01279163 -0.00395267 -0.00660236  0.00338493\n",
            "  0.05723305  0.04263638  0.04826457 -0.02185955 -0.08236013  0.0287613\n",
            " -0.04757535 -0.00422647 -0.08301193  0.06606403  0.04168128  0.04225906\n",
            "  0.03009798  0.05508625  0.02304784  0.02956334  0.02328967 -0.03885793\n",
            "  0.09149846 -0.09700762  0.05365682 -0.02802743  0.06186439  0.04086529\n",
            " -0.02531239  0.06880185  0.03396954 -0.05555064  0.02037002 -0.04165684\n",
            "  0.03669901  0.0640901   0.03570412 -0.00553594  0.04140023  0.02657003\n",
            " -0.06931031 -0.03869386 -0.04545971  0.01155922 -0.00488004  0.0426462\n",
            " -0.0636877   0.03799228 -0.02469095 -0.01786827 -0.11714919 -0.0975224\n",
            " -0.01709365 -0.05293322 -0.00160847  0.03877039 -0.06770448 -0.00353634\n",
            " -0.01539879 -0.00023611 -0.009705    0.0525001  -0.01574586 -0.05584005\n",
            "  0.02420623 -0.00578888  0.01579252 -0.07729022 -0.02444515 -0.02403441\n",
            " -0.01252517 -0.0195509   0.01120158 -0.01532311  0.09776177  0.07899044\n",
            " -0.01579785  0.03239097 -0.01639406  0.05716111  0.05955906  0.07648396\n",
            "  0.01000867 -0.00473776  0.04412794 -0.02058393  0.02995613  0.03681798\n",
            "  0.0209659   0.05551313 -0.00472502 -0.01202328  0.02113431 -0.01948468\n",
            " -0.05865138 -0.02253946  0.06847092  0.00293734 -0.03504635  0.01365006\n",
            " -0.09010956 -0.00802817 -0.04642696 -0.02624361 -0.06792872 -0.05286164\n",
            "  0.02717434  0.01057739  0.01564983 -0.06623219  0.06408169  0.03001962\n",
            " -0.01631466 -0.04925485 -0.02576832 -0.04653771 -0.01977751 -0.00355507\n",
            " -0.03372259 -0.07618748  0.02066366 -0.06482755  0.03217461 -0.01092269\n",
            " -0.01718775 -0.02406414  0.00717621  0.07962366  0.03695744  0.03045471\n",
            "  0.09488815  0.00117036  0.02282127  0.01545661  0.03193149 -0.03805847\n",
            " -0.03073642 -0.06083288 -0.02635645  0.0254594  -0.02669582  0.01615116\n",
            "  0.01507757  0.01439681  0.04895734  0.02372173 -0.04542394  0.06016143\n",
            "  0.00688547 -0.00513737  0.10314457  0.01167683 -0.04161828  0.01970115\n",
            " -0.07644531  0.07899482  0.02732771  0.02116577  0.05548168  0.03341798\n",
            "  0.04803425  0.02059413 -0.00346371  0.06049509  0.00493905 -0.04375328\n",
            "  0.01996547  0.03197958  0.04180827  0.02040025  0.05875337  0.01775215\n",
            "  0.03359037 -0.00116151 -0.03342745 -0.02892528 -0.01895843  0.05830195\n",
            "  0.03731528 -0.04010315  0.01020672  0.02329778 -0.04411007 -0.08327088\n",
            " -0.05110124  0.01217918  0.09359314 -0.02469934 -0.02297806 -0.03897167\n",
            "  0.02345902  0.00067792 -0.06096138 -0.01937438 -0.02764596  0.01525408\n",
            "  0.07700519 -0.03905027  0.00822655 -0.03271889 -0.07537898 -0.01849484\n",
            "  0.01702019  0.0040585   0.0699041   0.04940457 -0.06243282 -0.00178328\n",
            " -0.01065948 -0.00928921 -0.02235799 -0.05218275 -0.06756899  0.0613427\n",
            "  0.00699825 -0.03643901 -0.02644681 -0.01890211  0.02612516 -0.03388806\n",
            "  0.00105624  0.05355155 -0.00194111 -0.02571725 -0.04154576 -0.00361628\n",
            "  0.08862311  0.03117609  0.06117413  0.02730942 -0.08572201  0.08137289\n",
            "  0.00439804 -0.00416813 -0.01750178  0.01217526  0.00269837 -0.04631697\n",
            "  0.05641455 -0.00846761  0.04905433 -0.02528094  0.00193327 -0.02050643\n",
            " -0.02846969 -0.00645685  0.05126152  0.03582475  0.0033618  -0.04733974\n",
            "  0.02973489 -0.0122521  -0.03350455  0.06956115 -0.0429418   0.00483133\n",
            "  0.02245149 -0.00588596  0.02491498 -0.06521189  0.01002998  0.04180297\n",
            " -0.0244523   0.00798181 -0.05623091 -0.04452998  0.01560469  0.00986408\n",
            " -0.02810116 -0.01782225 -0.05889104 -0.0351436  -0.00859083  0.10779268\n",
            " -0.01023237  0.06080008 -0.10309302  0.01397533  0.02526072 -0.07942653\n",
            "  0.01960654  0.05659704  0.00549258 -0.09367355  0.0263099  -0.00264487\n",
            " -0.00870358  0.09565648  0.07037158  0.00666058  0.01199247  0.04150009\n",
            " -0.03298025  0.02638438  0.06660346 -0.05924815 -0.09617335  0.01599596\n",
            "  0.03302556 -0.00165851  0.00336149 -0.00456443  0.00200567 -0.02196456\n",
            " -0.09801189 -0.00093401  0.09824389 -0.00995049  0.00316581  0.03275843\n",
            " -0.00061821  0.01058625  0.00352063 -0.02993771  0.03894569  0.05737574\n",
            " -0.10843383  0.02968427  0.05140879 -0.06388002  0.01988242 -0.04600152\n",
            " -0.03955155  0.00290028  0.07266345 -0.01782443 -0.0110463  -0.0075507\n",
            "  0.03013197  0.01059503 -0.03528689 -0.004358    0.02702685 -0.01792463\n",
            "  0.00025843  0.06066977  0.06143346  0.03103392 -0.02381746 -0.01173486\n",
            "  0.04217441 -0.06347054]\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "image_path = \"./images/q2.jpg\"\n",
        "q2 = create_image_embedding(image_path)\n",
        "\n",
        "# 'embedding' now contains a dense vector representation of the image\n",
        "print(\"Image Embedding Shape:\", q2.shape)\n",
        "print(\"Image Embedding:\", q2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XmPsgV__Z5D-",
        "outputId": "c20e5d3d-93c7-4130-d836-5d1e5a85bcef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Embedding Shape: (512,)\n",
            "Image Embedding: [-1.11871092e-02 -1.03831679e-01 -6.49788529e-02  6.26160726e-02\n",
            "  5.31825311e-02  2.93166805e-02  2.85817944e-02  1.07557606e-02\n",
            "  2.30576918e-02  2.11645626e-02  4.49966975e-02 -1.05079694e-03\n",
            "  5.12558147e-02 -1.57793239e-02  2.27004047e-02  4.33514714e-02\n",
            " -4.31503728e-02  3.46557088e-02  3.98752280e-02  4.01685536e-02\n",
            "  1.22629656e-02  3.47619690e-02  3.74376588e-02 -2.92636734e-02\n",
            " -1.95951890e-02  7.84217790e-02 -1.65973324e-02 -7.39309639e-02\n",
            "  1.87867340e-02 -3.93897295e-02 -3.14862356e-02  2.39189509e-02\n",
            " -2.03516819e-02 -3.26003097e-02  3.72588672e-02  1.85265020e-02\n",
            "  4.76108380e-02 -1.18853813e-02 -3.72915268e-02 -1.12608625e-02\n",
            " -1.34668415e-02  2.98970081e-02 -4.51772660e-02 -4.26033661e-02\n",
            " -5.81922904e-02 -1.54332519e-02  4.68713380e-02  8.94587934e-02\n",
            " -5.68718724e-02  1.97885297e-02 -1.02427714e-02  3.88370454e-02\n",
            "  1.36065371e-02  3.18910480e-02 -3.45597640e-02  1.28371874e-02\n",
            "  4.65074666e-02  5.49756736e-02 -4.13392968e-02 -1.22491121e-02\n",
            "  8.02586153e-02  3.81988622e-02 -5.95349297e-02 -4.23814692e-02\n",
            " -3.49300839e-02 -1.36340754e-02  1.65573079e-02 -9.17057693e-02\n",
            "  4.65478487e-02  4.74346522e-03 -7.06184953e-02  3.83565389e-02\n",
            " -2.63459440e-02 -2.60344408e-02  3.11261583e-02  1.57261752e-02\n",
            " -2.50283480e-02 -3.79884839e-02 -4.75553572e-02 -9.30941955e-04\n",
            "  2.56992294e-03 -1.16410665e-02 -2.28207912e-02 -1.37971453e-02\n",
            "  1.74771342e-03  3.25816385e-02  7.12756664e-02  8.06502923e-02\n",
            " -8.84141400e-02  3.10803317e-02  1.12167420e-02 -5.51299080e-02\n",
            "  2.68588867e-02  5.49648777e-02  5.49707972e-02  3.85880768e-02\n",
            "  6.21338785e-02 -4.84648123e-02 -8.64821225e-02  3.40486951e-02\n",
            " -1.84795298e-02 -1.02242075e-01  5.26406690e-02  1.47886546e-02\n",
            " -1.00119589e-02 -9.87762213e-02 -3.48315723e-02 -1.60418544e-02\n",
            "  5.70422830e-03  1.07697593e-02  4.71926183e-02  2.86004227e-02\n",
            " -7.81947225e-02 -1.11674763e-01 -2.46943682e-02 -6.18855916e-02\n",
            "  4.04402800e-02 -5.76940831e-03 -3.66969779e-02  8.73905346e-02\n",
            " -1.95407961e-03 -7.69524351e-02  2.24794121e-03  4.49692048e-02\n",
            " -3.32978033e-02 -2.64183376e-02 -7.62605900e-03 -4.31122668e-02\n",
            " -4.82989699e-02 -4.19171266e-02  6.26357496e-02 -1.28543442e-02\n",
            "  3.03642172e-02 -6.85790405e-02 -7.44268671e-02  4.44427580e-02\n",
            "  4.89318557e-02  7.48089999e-02  5.43042049e-02  6.00816235e-02\n",
            " -7.12566264e-03  8.05306341e-03  3.31095867e-02  7.52047971e-02\n",
            "  6.79623038e-02 -1.45985046e-02  1.66600458e-02  1.66200865e-02\n",
            " -1.90182421e-02 -2.40901653e-02  3.60215753e-02 -4.52494761e-03\n",
            " -2.01554149e-02 -3.92694548e-02 -6.22260990e-03 -6.53752685e-02\n",
            "  2.25214642e-02  1.63132250e-01  2.94811204e-02  1.35595500e-02\n",
            "  1.56537015e-02 -4.26869951e-02 -5.20180166e-02  2.30753869e-02\n",
            "  2.58502923e-02  4.04522233e-02 -7.47844577e-02 -9.00941938e-02\n",
            "  5.50987013e-02  1.23147229e-02  6.08907044e-02 -7.16004241e-03\n",
            "  5.38712330e-02 -8.02025273e-02  5.18828854e-02 -4.00018580e-02\n",
            " -5.88896498e-02 -1.56445596e-02 -6.38517365e-02  7.16072917e-02\n",
            " -9.03155804e-02  2.27934793e-02 -6.62763342e-02  6.39582053e-04\n",
            "  2.69628260e-02  5.58275171e-02  7.03577921e-02 -6.42892485e-03\n",
            " -7.07693622e-02 -5.25698066e-02 -2.28427649e-02 -1.37585355e-02\n",
            "  5.22842593e-02 -3.93862315e-02 -1.84570011e-02 -1.30426968e-02\n",
            " -1.33173876e-02  1.35453995e-02 -2.80390326e-02  5.79056144e-02\n",
            "  7.42083117e-02  4.26574275e-02 -7.73473307e-02 -7.72884861e-02\n",
            " -6.86476380e-02  4.26337533e-02 -3.17118168e-02 -3.85838673e-02\n",
            " -3.27074490e-02  2.25020368e-02 -5.35984933e-02 -8.51002261e-02\n",
            " -2.57829379e-04 -5.79836592e-02 -4.83230464e-02  6.95719942e-02\n",
            " -5.70446579e-03  3.79940495e-02  3.91066223e-02  8.65514018e-03\n",
            " -1.94135103e-02 -8.58654976e-02 -2.17920132e-02 -3.61560769e-02\n",
            "  1.96798276e-02  7.68107409e-03 -4.60527949e-02 -6.16294369e-02\n",
            "  1.40599320e-02 -1.96032431e-02  3.69630717e-02  3.76747772e-02\n",
            " -3.93913011e-04 -5.43305613e-02 -4.82097920e-03  7.32045919e-02\n",
            "  2.96099130e-02 -1.40450243e-02 -7.42208771e-03 -1.03640500e-02\n",
            " -3.25925536e-02 -3.01711783e-02 -1.41784605e-02  1.91026032e-02\n",
            "  1.19687105e-02  5.49073629e-02 -1.27675049e-02  1.89592652e-02\n",
            " -3.60129704e-03  4.07527685e-02  1.58893522e-02 -3.44600640e-02\n",
            "  1.06150880e-02  3.84745449e-02  6.57472461e-02  3.69573571e-02\n",
            " -7.59413615e-02  7.12694554e-03  1.02292381e-01  3.07430904e-02\n",
            " -1.42631624e-02 -3.66568081e-02  4.19115573e-02  2.91144066e-02\n",
            " -2.39088945e-02 -1.75305735e-02  2.30969545e-02  1.10883974e-02\n",
            " -2.20475420e-02  9.80181340e-03  5.87587245e-03  3.13788243e-02\n",
            " -6.55176118e-02  2.09741853e-03  5.72765693e-02 -1.58681860e-03\n",
            " -4.68703508e-02 -1.52097307e-02 -7.16491938e-02  2.99625471e-02\n",
            " -6.92216959e-03 -3.35283726e-02 -2.81945970e-02 -3.25089544e-02\n",
            " -6.62789941e-02 -4.97282185e-02 -3.42484340e-02 -2.92028184e-03\n",
            " -3.12268082e-02 -2.36473512e-03  6.91680908e-02 -5.72880507e-02\n",
            "  2.19719894e-02  7.09659457e-02 -9.11697820e-02  8.05146247e-03\n",
            " -3.67003754e-02 -3.51633504e-02 -3.51897813e-02 -4.24067210e-03\n",
            "  7.31516723e-03  3.96723077e-02  1.22136600e-01  9.59074348e-02\n",
            "  3.37592028e-02 -8.30706581e-02 -4.62482125e-02  5.29494435e-02\n",
            " -6.37108311e-02 -7.88331628e-02  2.41478123e-02  3.51880379e-02\n",
            "  3.10061988e-03 -8.55734048e-04  9.33466479e-03  9.05917734e-02\n",
            " -5.29304240e-03 -5.91451349e-03 -3.49753648e-02 -3.79948057e-02\n",
            " -1.01735713e-02  1.44569213e-02 -3.40275578e-02  5.79419993e-02\n",
            " -6.94063231e-02  5.42249419e-02  4.10679402e-03 -2.00269953e-03\n",
            " -3.38053657e-03  5.25264814e-02  8.21771193e-03 -1.42585263e-02\n",
            "  3.32685071e-03  4.92198113e-03 -6.54617976e-03  1.02857854e-02\n",
            "  8.78617838e-02 -5.40974829e-03 -4.90192790e-03 -2.69505940e-02\n",
            "  1.16487309e-01 -1.11756781e-02 -5.35591925e-03  1.03215575e-02\n",
            "  9.29426774e-03 -6.83753490e-02 -5.87190082e-03  5.61574586e-02\n",
            " -2.96462979e-02 -1.24421641e-02 -2.96109170e-02  3.14039290e-02\n",
            "  1.07058501e-02  1.78728383e-02  1.22983707e-02  3.82478572e-02\n",
            "  8.27867389e-02 -2.93790568e-02  8.19675811e-03  3.53541337e-02\n",
            "  1.54894055e-03  6.46627173e-02  1.66487310e-03 -4.24213782e-02\n",
            " -7.26400912e-02  6.79494515e-02  8.68740026e-03 -8.92815646e-03\n",
            "  2.97773303e-03 -3.14534493e-02 -2.34413091e-02  2.59234216e-02\n",
            " -3.53894010e-02  1.58237852e-02 -9.52033792e-03 -1.04601365e-02\n",
            " -3.72940972e-02  9.96353384e-03  3.83030740e-03  1.09600406e-02\n",
            " -2.67700199e-03  3.83338295e-02  6.69002682e-02  5.93743697e-02\n",
            "  1.16633736e-01 -6.55779615e-02 -1.49554471e-02 -3.90407965e-02\n",
            " -3.73179391e-02  3.14061157e-02  5.28097264e-02 -3.25627774e-02\n",
            " -3.03207189e-02 -4.85909283e-02 -3.29659134e-02 -2.05135643e-02\n",
            "  2.61528362e-02  3.25204283e-02  3.23005988e-05 -2.75909379e-02\n",
            " -6.97963536e-02 -3.12910601e-03 -1.11005129e-02  1.16420761e-02\n",
            " -8.27886760e-02  5.85027412e-02 -1.77270323e-02 -2.46270373e-02\n",
            " -1.12117054e-02 -4.60258499e-02  5.06376028e-02 -3.59613001e-02\n",
            "  9.49118193e-03 -3.65367532e-02 -6.32456467e-02 -7.98258279e-03\n",
            "  1.70360636e-02  1.34473937e-02  9.58526693e-03 -5.80657721e-02\n",
            "  4.14541624e-02 -3.09430752e-02 -1.94751471e-02  4.93912958e-02\n",
            "  6.71683857e-03  5.04589938e-02  4.60711122e-02  8.14810675e-03\n",
            "  4.14786424e-04 -5.30870110e-02  4.03976515e-02  3.44096720e-02\n",
            "  2.18358040e-02  8.94732028e-03 -8.08790401e-02  1.38119273e-02\n",
            "  4.42448147e-02  2.45676450e-02 -8.20356831e-02 -2.84231603e-02\n",
            " -2.38482263e-02  4.76225354e-02 -6.49007410e-02  4.62613478e-02\n",
            "  1.12036755e-02 -6.22173846e-02 -2.37915777e-02  1.36410967e-02\n",
            " -5.50070927e-02 -4.87210602e-03  4.57302146e-02  3.77252176e-02\n",
            "  4.11336459e-02 -6.46088272e-03  7.15470240e-02 -1.30598292e-01\n",
            "  5.94469868e-02  5.49982078e-02  6.16098642e-02  4.15492579e-02\n",
            " -1.65865161e-02  9.11596268e-02  1.50168948e-02  5.86488023e-02\n",
            "  3.52287665e-02  5.36219515e-02 -2.27170866e-02  2.38616634e-02\n",
            "  5.69305196e-02 -5.61067089e-02 -1.63862202e-02 -7.68495025e-03\n",
            " -7.35249231e-03 -2.82694027e-02 -2.63305176e-02 -1.63035244e-02\n",
            " -2.63792966e-02 -2.06475402e-03 -3.17978999e-03 -1.68414932e-04\n",
            " -3.67765166e-02  8.34158901e-03  9.04802233e-02 -5.55233611e-03\n",
            "  5.70693016e-02  4.57675941e-02  6.21742709e-03  8.65672948e-04\n",
            " -4.08265274e-03 -8.40259567e-02  1.97605304e-02 -2.01635025e-02\n",
            "  5.56236785e-03  6.52863011e-02  1.03389621e-01  2.02528182e-02\n",
            "  1.90992840e-02 -2.93199196e-02 -1.71488412e-02 -4.66644801e-02\n",
            " -6.03761850e-03 -1.08810216e-02 -1.04834288e-02 -1.31510189e-02\n",
            " -3.51735875e-02 -1.67632438e-02  4.90136743e-02 -1.90571100e-02\n",
            "  2.45440360e-02  1.26176523e-02 -3.38700116e-02 -2.57172789e-02]\n"
          ]
        }
      ],
      "source": [
        "image_path = \"./images/s2.jpg\"\n",
        "cat2 = create_image_embedding(image_path)\n",
        "\n",
        "# 'embedding' now contains a dense vector representation of the image\n",
        "print(\"Image Embedding Shape:\", cat2.shape)\n",
        "print(\"Image Embedding:\", cat2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qGsGyHm7Z7GG"
      },
      "outputs": [],
      "source": [
        "q1 = create_image_embedding(\"./images/q1.jpg\")\n",
        "q2 = create_image_embedding(\"./images/q2.jpg\")\n",
        "s1 = create_image_embedding(\"./images/s1.jpg\")\n",
        "s2 = create_image_embedding(\"./images/s2.jpg\")\n",
        "z1 = create_image_embedding(\"./images/z1.jpg\")\n",
        "z2 = create_image_embedding(\"./images/z2.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EaGMoOeMatEn",
        "outputId": "c28011de-2850-4cf3-fadc-fd904eedf9c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: milvus-lite in /usr/local/lib/python3.10/dist-packages (2.4.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from milvus-lite) (4.67.1)\n",
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (75.1.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.29.2)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.0.1)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.2.2)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.4.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U milvus-lite\n",
        "\n",
        "!pip install -U pymilvus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8VCan5cjbCBW"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient\n",
        "client = MilvusClient(\"./milvus_demo.db\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lhSATnNMaZmf"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient\n",
        "import numpy as np\n",
        "\n",
        "client = MilvusClient(\"./milvus_demo.db\")\n",
        "client.create_collection(\n",
        "    collection_name=\"images\",\n",
        "    dimension=512  # The vectors we will use in this demo has 384 dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bB-wjZmIarV3"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    {\"id\": 1, \"person_name\": \"Qasim\", \"vector\": q1},\n",
        "    {\"id\": 2, \"person_name\": \"Qasim\", \"vector\": q2},\n",
        "    {\"id\": 3, \"person_name\": \"Shahzad\", \"vector\": s1},\n",
        "    {\"id\": 4, \"person_name\": \"Shahzad\", \"vector\": s2},\n",
        "    {\"id\": 5, \"person_name\": \"Zia Khan\", \"vector\": z1},\n",
        "    {\"id\": 6, \"person_name\": \"Zia Khan\", \"vector\": z2}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pPFMGNspcEnF"
      },
      "outputs": [],
      "source": [
        "res = client.insert(\n",
        "    collection_name=\"images\",\n",
        "    data=data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0E3TbHNIaZsF",
        "outputId": "1f700ce5-0166-4bd9-b1b6-9f24cf1deeae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: [\"[{'id': 3, 'distance': 1.0, 'entity': {'person_name': 'Shahzad', 'id': 3}}]\"] \n"
          ]
        }
      ],
      "source": [
        "res = client.search(\n",
        "    collection_name=\"images\",\n",
        "    data=[s1],\n",
        "    limit=1,\n",
        "    output_fields=[\"id\", \"person_name\"],\n",
        ")\n",
        "print(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CWjC3uQyaZ0h",
        "outputId": "7258e446-e9ce-4335-c593-8533374079d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.24817514e-02, -6.95521683e-02, -5.71145639e-02,  1.31000075e-02,\n",
              "       -1.47810532e-02,  7.16738496e-03, -2.34999787e-02,  1.97612699e-02,\n",
              "       -4.16584350e-02, -2.91828113e-03,  2.64127906e-02, -3.83068211e-02,\n",
              "        1.42124025e-02, -8.85221642e-03,  6.74236938e-02,  1.89457219e-02,\n",
              "       -1.88124925e-02,  2.76937392e-02, -2.05918774e-02,  3.28397602e-02,\n",
              "        2.54747327e-02,  6.51649237e-02,  5.91187514e-02, -1.37622254e-02,\n",
              "       -1.62290614e-02,  7.27363154e-02,  2.17340104e-02, -6.91848844e-02,\n",
              "       -3.96975875e-02, -7.55026042e-02,  3.57599705e-02,  1.21188760e-01,\n",
              "       -1.83019303e-02,  8.30751806e-02, -6.06450113e-03, -2.25774348e-02,\n",
              "        3.53784598e-02, -1.35246236e-02, -4.16965708e-02, -6.32572472e-02,\n",
              "        5.14834281e-03, -5.88279739e-02,  9.78095364e-03, -6.04344066e-03,\n",
              "       -3.18242772e-03, -2.91550420e-02,  1.14479326e-02, -2.86392476e-02,\n",
              "       -7.80728087e-02,  3.53836943e-03,  3.22798267e-02,  9.29585006e-03,\n",
              "        9.00852680e-03, -2.33224593e-03, -4.13830094e-02, -7.94971082e-03,\n",
              "       -2.96882242e-02, -4.30468982e-03,  5.67418672e-02,  3.27768959e-02,\n",
              "        4.29983297e-03,  3.70545313e-02, -5.94458506e-02, -8.84543918e-03,\n",
              "        5.85726788e-03,  2.22915933e-02, -4.36792010e-03, -6.72260448e-02,\n",
              "        6.59945160e-02,  1.37662059e-02, -4.84879240e-02, -6.09320924e-02,\n",
              "        4.99185696e-02,  4.51833867e-02,  3.24278325e-02,  4.11183685e-02,\n",
              "       -5.70038939e-03, -7.65118524e-02, -4.01294045e-02, -2.37991046e-02,\n",
              "        7.56736472e-02,  5.51540889e-02, -2.96706390e-02, -2.84051262e-02,\n",
              "       -5.90951601e-03,  2.56186910e-02,  2.94693001e-02,  5.83402887e-02,\n",
              "       -5.92357069e-02, -1.55729614e-02, -7.97903985e-02,  1.18069379e-02,\n",
              "        6.27558008e-02,  5.02642840e-02, -2.14037579e-02,  3.72140273e-03,\n",
              "       -4.52313647e-02, -3.32902111e-02, -4.64059860e-02, -2.49079559e-02,\n",
              "       -2.07081597e-04,  3.97281349e-02,  5.06505258e-02, -1.25937304e-02,\n",
              "        1.18481917e-02, -4.97891530e-02,  6.80403551e-03, -1.33343458e-01,\n",
              "       -8.54023471e-02, -5.63570634e-02, -3.41914967e-02, -3.52256373e-02,\n",
              "       -6.71961531e-02, -5.64476810e-02, -1.33338608e-02, -2.14867759e-02,\n",
              "       -4.05299179e-05, -2.35045757e-02, -6.50116801e-02,  7.27949888e-02,\n",
              "       -1.79477446e-02,  7.39488285e-03, -4.27512228e-02,  6.00999705e-02,\n",
              "       -7.08834827e-02, -5.02809584e-02,  2.16119122e-02, -2.08453089e-02,\n",
              "        9.71989036e-02, -5.17645851e-02,  4.45156023e-02, -3.41355428e-02,\n",
              "       -9.84896161e-03, -1.32749043e-02, -3.07411123e-02,  2.57842913e-02,\n",
              "        2.01335121e-02, -7.67689645e-02, -8.23393755e-04,  1.01570403e-02,\n",
              "        4.30968292e-02,  4.31826189e-02, -1.19230179e-02,  1.35730132e-02,\n",
              "        3.70394737e-02, -3.91320065e-02, -3.77360508e-02, -1.93935111e-02,\n",
              "        7.26344436e-02,  4.94882911e-02, -2.33351756e-02,  8.39128066e-03,\n",
              "       -4.95436974e-03,  6.53983559e-03,  2.03407276e-02, -8.06430504e-02,\n",
              "       -3.87963764e-02,  1.31322481e-02,  2.98163970e-03, -8.74795914e-02,\n",
              "       -4.32164036e-03, -1.80535042e-03,  5.71280392e-03,  2.62935758e-02,\n",
              "        7.53209293e-02, -4.12652120e-02, -1.20755069e-01,  6.79069152e-03,\n",
              "        4.17256057e-02, -2.67583001e-02,  3.12401522e-02,  3.94094214e-02,\n",
              "       -6.75784722e-02,  2.33525746e-02, -3.41195241e-02, -2.33223084e-02,\n",
              "       -6.91020712e-02,  7.29102120e-02, -5.02626114e-02,  1.25323161e-01,\n",
              "       -1.17201982e-02,  4.33827639e-02,  1.22177890e-02, -2.99049653e-02,\n",
              "        3.99019793e-02, -1.45023558e-02,  3.93009968e-02,  1.50658619e-02,\n",
              "        1.58603396e-02,  1.69359799e-02,  5.35686985e-02,  1.22422492e-02,\n",
              "        5.95494248e-02,  1.17672039e-02,  1.41550098e-02,  5.65672666e-02,\n",
              "        9.30546876e-03,  2.29529347e-02,  1.76011156e-02,  2.36361176e-02,\n",
              "        1.30705118e-01,  2.77112275e-02,  2.90619489e-02,  4.00389656e-02,\n",
              "       -7.38896197e-04,  2.84960382e-02,  3.15733887e-02,  1.91254076e-02,\n",
              "       -3.95908095e-02,  7.73507357e-02,  4.76462953e-02,  1.17336279e-02,\n",
              "       -2.91017406e-02,  9.86644700e-02, -5.50161526e-02,  5.44534735e-02,\n",
              "        5.00231646e-02, -5.96228577e-02, -2.01421212e-02, -1.01749669e-03,\n",
              "        8.91034957e-03,  1.60957836e-02,  3.77411023e-02,  5.79937249e-02,\n",
              "        6.39120862e-02, -3.70233087e-03, -2.87711620e-02,  2.09271559e-04,\n",
              "        2.51285210e-02,  2.84892879e-02,  2.78198048e-02, -1.28404973e-02,\n",
              "        7.42592104e-03, -2.46014390e-02, -3.44216451e-02,  1.62805542e-02,\n",
              "       -4.79483269e-02,  1.35598902e-03,  6.16448782e-02, -6.87600300e-02,\n",
              "        1.82381254e-02,  2.63386872e-04,  1.26111498e-02,  1.05135972e-02,\n",
              "        1.54179230e-03,  9.85087380e-02, -9.94219724e-03,  3.28031443e-02,\n",
              "        4.08536159e-02,  7.81174153e-02, -6.62003383e-02,  8.40492081e-03,\n",
              "        5.02250064e-03, -7.10864216e-02,  2.34019738e-02, -8.12415108e-02,\n",
              "       -1.28225172e-02, -1.22457109e-02,  6.33021293e-04, -6.15240484e-02,\n",
              "        1.24748200e-02, -4.76352274e-02, -2.32294458e-03, -2.21364126e-02,\n",
              "       -1.66784897e-02, -4.95590605e-02,  1.22929895e-02, -3.40424962e-02,\n",
              "        3.41178686e-03, -4.21307608e-02,  2.98470650e-02,  2.28803419e-02,\n",
              "        1.29751740e-02, -1.09783541e-02,  4.56995144e-02,  5.56227416e-02,\n",
              "       -5.19800670e-02, -4.33722511e-02, -1.71272866e-02,  1.16889320e-01,\n",
              "       -6.52210321e-03, -3.45687568e-02, -7.37873092e-02, -9.02424753e-02,\n",
              "        8.02264642e-03,  2.42194254e-03, -3.10338810e-02, -5.04924506e-02,\n",
              "       -1.89947212e-04,  6.55261278e-02,  2.15384476e-02,  6.45241141e-02,\n",
              "        5.84091656e-02,  1.40737221e-02, -1.88145228e-02, -2.73078796e-03,\n",
              "       -4.32309434e-02, -4.99294251e-02,  1.64762721e-03,  1.75675973e-02,\n",
              "        1.80116612e-02, -3.02673085e-03,  2.86333561e-02,  2.04871837e-02,\n",
              "        1.95416082e-02, -3.42075899e-02, -4.00706381e-02,  1.74950771e-02,\n",
              "       -3.10321338e-02, -3.78147922e-02, -4.58726995e-02,  5.14653772e-02,\n",
              "        9.03381314e-03, -6.23765849e-02, -1.26071125e-02,  7.62492046e-02,\n",
              "        1.53933102e-02,  1.11900121e-02,  1.16757199e-01, -2.79977378e-02,\n",
              "        2.20932662e-02, -9.56923291e-02, -1.54083027e-02, -2.06088740e-02,\n",
              "       -1.24194724e-02,  1.41271222e-02, -2.56264508e-02,  2.69593261e-02,\n",
              "       -2.79591195e-02,  2.30461378e-02,  1.92337707e-02,  2.69281548e-02,\n",
              "       -3.16766612e-02,  1.17320297e-02,  2.11195219e-02,  5.30175567e-02,\n",
              "        7.31225079e-03, -7.88969770e-02,  4.67875740e-04, -9.24810097e-02,\n",
              "        3.13139260e-02, -4.64247027e-03, -1.48922708e-02, -4.25154567e-02,\n",
              "        1.71921924e-02,  2.31326651e-03,  2.88056172e-02,  6.31904602e-02,\n",
              "       -4.44601700e-02,  6.56732395e-02, -1.49543434e-02, -5.32289706e-02,\n",
              "        3.64394882e-03,  2.26965416e-02,  5.85365109e-02,  7.91208595e-02,\n",
              "       -2.87477244e-02, -7.43010193e-02, -5.65719791e-02,  5.58402836e-02,\n",
              "        2.07560007e-02, -2.06331629e-02,  2.08277330e-02, -4.84098643e-02,\n",
              "       -1.70102920e-02,  4.30889922e-04,  1.35695534e-02,  1.61890294e-02,\n",
              "       -1.57892727e-03, -7.84217194e-02,  1.46684460e-02,  4.81098667e-02,\n",
              "       -3.75466608e-02, -1.47064449e-03, -6.43187612e-02,  4.80592437e-02,\n",
              "       -5.71908578e-02,  7.31672198e-02,  7.78926583e-03, -4.90363687e-03,\n",
              "       -7.12692663e-02,  2.68223993e-02,  5.82082458e-02,  4.49732272e-03,\n",
              "        5.45850210e-02, -6.48914874e-02,  1.01438453e-02, -8.65698047e-03,\n",
              "       -4.28325869e-03, -1.86181962e-02, -2.00186707e-02, -3.07403542e-02,\n",
              "        2.36888342e-02, -2.11714152e-02,  1.12299598e-03,  1.63156976e-04,\n",
              "       -3.28554437e-02,  9.74753127e-02, -5.48664406e-02, -6.74583614e-02,\n",
              "       -1.74198989e-02, -3.74216773e-02,  2.74996050e-02,  6.89690793e-03,\n",
              "       -3.51220779e-02, -5.69205219e-03,  5.87210525e-03, -4.24418561e-02,\n",
              "       -1.71293356e-02,  1.03775989e-02,  7.93411136e-02, -5.97649664e-02,\n",
              "        8.60787705e-02, -5.34234429e-03, -4.54051569e-02,  4.09873985e-02,\n",
              "        3.23377699e-02, -5.55171221e-02, -3.10906153e-02, -2.15111692e-02,\n",
              "        1.42967775e-02, -3.87201495e-02, -3.34603228e-02,  3.37757953e-02,\n",
              "       -3.23240869e-02, -7.72664417e-03, -5.44146926e-04,  2.56605763e-02,\n",
              "        6.65915981e-02, -8.60606208e-02, -3.47773954e-02, -4.25960869e-02,\n",
              "       -3.04872636e-02, -1.02128007e-01,  4.46309522e-02,  3.33913900e-02,\n",
              "        4.79580499e-02, -2.64893062e-02, -7.01621827e-03, -1.46806417e-02,\n",
              "       -6.87274858e-02,  2.62111854e-02, -3.50053348e-02,  8.77641663e-02,\n",
              "        4.65720296e-02, -4.34260331e-02, -9.77596827e-03,  2.81278025e-02,\n",
              "        2.62254886e-02,  2.19770614e-02,  1.43098310e-02,  5.49791828e-02,\n",
              "        2.28749559e-04, -1.33595187e-02, -1.33083509e-02, -1.11339483e-02,\n",
              "       -6.75122216e-02,  4.79406714e-02,  5.87920509e-02,  1.11965925e-01,\n",
              "       -5.93853462e-03,  6.06689043e-02,  7.09746331e-02,  1.73289031e-02,\n",
              "       -3.09793558e-02, -2.43727788e-02, -4.57762182e-02, -1.12846745e-02,\n",
              "        6.73478097e-02,  3.60109322e-02,  3.91490348e-02,  7.59171695e-02,\n",
              "       -2.17572376e-02, -4.12728265e-03, -3.74080501e-02, -3.78024951e-02,\n",
              "       -5.22801876e-02, -1.25557492e-02,  9.92167369e-03, -2.69970074e-02,\n",
              "       -6.94635659e-02, -5.37931472e-02,  4.25067469e-02,  2.72683930e-02,\n",
              "        2.39113756e-02, -8.74974951e-02, -2.15609074e-02,  4.34292145e-02,\n",
              "        2.17814581e-03, -9.71320868e-02,  2.09667608e-02, -1.02569520e-01,\n",
              "        7.03590438e-02,  3.38974036e-02,  7.70312641e-03, -5.49269877e-02,\n",
              "        9.60583240e-02, -1.44448215e-02,  7.83233494e-02, -3.96749787e-02,\n",
              "        3.63753662e-02,  1.50430491e-02, -1.04491107e-01,  2.02961024e-02,\n",
              "       -5.13066724e-02, -1.44733312e-02,  1.40453633e-02, -4.04716879e-02,\n",
              "       -5.72112873e-02,  6.12239875e-02,  1.66096389e-02, -1.76797435e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "q3 = create_image_embedding('./images/q3.jpg')\n",
        "q3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Pr7oM5Otf2My",
        "outputId": "540e53d7-eb51-403c-e87b-432929037352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: [\"[{'id': 1, 'distance': 0.5428284406661987, 'entity': {'person_name': 'Qasim', 'id': 1}}, {'id': 4, 'distance': 0.2969600558280945, 'entity': {'person_name': 'Shahzad', 'id': 4}}]\"] \n"
          ]
        }
      ],
      "source": [
        "res = client.search(\n",
        "    collection_name=\"images\",\n",
        "    data=[q3],\n",
        "    limit=2,\n",
        "    output_fields=[\"id\", \"person_name\"],\n",
        ")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaxs40aDamtV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}